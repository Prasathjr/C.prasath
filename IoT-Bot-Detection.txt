In [1]:
from IPython.display import display, HTML
In [2]:
display(HTML("""
<style>
 .messagebox{
     border-radius: 2px;
     padding: 1.25em 1.5em;
     border: 1px solid;
 }
.messagelightgreen{
     border-color: hsl(164deg 95% 38%);
     color: rgb(5 139 102);
     background-color: rgb(236 255 250);
 }
 .messagelightgreen b{
     color:rgb(139 77 5);
 }
 .messagebrown{
     border-color: hsl(35deg 96% 62%);
    color: rgb(143 84 4);
    background-color: rgb(255 245 234);
 }
 .messagebrown b{
     color: rgb(5 139 102);
 }
</style>"""))
In [3]:
import pandas as pd
In [4]:
pd.set_option('display.max_columns', 9999999999)
In [5]:
import numpy as np
In [6]:
import plotly.express as px
C:\ProgramData\Anaconda3\envs\python3.9.12\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
In [7]:
from sklearn.preprocessing import MinMaxScaler
In [8]:
import seaborn as sns
In [9]:
import matplotlib.pyplot as plt
In [10]:
import matplotlib as mpl
In [11]:
plt.rcParams["patch.force_edgecolor"] = True
plt.style.use('fivethirtyeight')
mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)
In [12]:
%matplotlib inline
In [13]:
from sklearn.metrics import confusion_matrix
In [14]:
from sklearn.metrics import silhouette_samples, silhouette_score
In [15]:
from sklearn import preprocessing, model_selection, metrics, feature_selection
In [16]:
from sklearn.model_selection import GridSearchCV, learning_curve
In [17]:
from sklearn.svm import SVC
In [18]:
from sklearn.preprocessing import StandardScaler
In [19]:
from sklearn.model_selection import train_test_split
In [20]:
from sklearn import neighbors, linear_model, svm, tree, ensemble
In [21]:
from sklearn.ensemble import RandomForestClassifier
In [22]:
from sklearn.svm import SVC, LinearSVC
In [23]:
from sklearn.linear_model import LogisticRegression
In [24]:
from sklearn.feature_selection import f_classif
In [25]:
from sklearn.metrics import accuracy_score
In [26]:
from sklearn.model_selection import KFold
In [27]:
from sklearn.model_selection import cross_val_score
In [28]:
from sklearn.model_selection import cross_val_predict
In [29]:
from sklearn.metrics import confusion_matrix
In [30]:
from sklearn.preprocessing import LabelEncoder
In [31]:
from tensorflow.keras.models import Sequential
In [32]:
from tensorflow.keras.layers import Dense, Activation,Conv1D,LSTM,Flatten
In [33]:
from tensorflow.keras.callbacks import EarlyStopping
In [34]:
from tensorflow.keras.utils import to_categorical, plot_model
In [35]:
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
In [36]:
import warnings
warnings.filterwarnings("ignore")
In [37]:
import itertools
In [38]:
import glob
In [39]:
import os
In [40]:
mirai_df_list = []
In [41]:
rows=10000
In [42]:
for file in glob.glob("Dataset/mirai/*.csv"):
    tmp_df = pd.read_csv(file, nrows=rows)
    tmp_df["target"] = "mirai-"+ os.path.splitext(os.path.basename(file))[0]
    mirai_df_list.append(tmp_df)
In [43]:
mirai_df_list[0].head()
Out[43]:
MI_dir_L5_weightMI_dir_L5_meanMI_dir_L5_varianceMI_dir_L3_weightMI_dir_L3_meanMI_dir_L3_varianceMI_dir_L1_weightMI_dir_L1_meanMI_dir_L1_varianceMI_dir_L0.1_weightMI_dir_L0.1_meanMI_dir_L0.1_varianceMI_dir_L0.01_weightMI_dir_L0.01_meanMI_dir_L0.01_varianceH_L5_weightH_L5_meanH_L5_varianceH_L3_weightH_L3_meanH_L3_varianceH_L1_weightH_L1_meanH_L1_varianceH_L0.1_weightH_L0.1_meanH_L0.1_varianceH_L0.01_weightH_L0.01_meanH_L0.01_varianceHH_L5_weightHH_L5_meanHH_L5_stdHH_L5_magnitudeHH_L5_radiusHH_L5_covarianceHH_L5_pccHH_L3_weightHH_L3_meanHH_L3_stdHH_L3_magnitudeHH_L3_radiusHH_L3_covarianceHH_L3_pccHH_L1_weightHH_L1_meanHH_L1_stdHH_L1_magnitudeHH_L1_radiusHH_L1_covarianceHH_L1_pccHH_L0.1_weightHH_L0.1_meanHH_L0.1_stdHH_L0.1_magnitudeHH_L0.1_radiusHH_L0.1_covarianceHH_L0.1_pccHH_L0.01_weightHH_L0.01_meanHH_L0.01_stdHH_L0.01_magnitudeHH_L0.01_radius HH_L0.01_covariance HH_L0.01_pcc HH_jit_L5_weight HH_jit_L5_mean HH_jit_L5_variance HH_jit_L3_weight HH_jit_L3_mean HH_jit_L3_variance HH_jit_L1_weight HH_jit_L1_mean HH_jit_L1_variance HH_jit_L0.1_weight HH_jit_L0.1_mean HH_jit_L0.1_variance HH_jit_L0.01_weight HH_jit_L0.01_mean HH_jit_L0.01_variance HpHp_L5_weight HpHp_L5_mean HpHp_L5_std HpHp_L5_magnitude HpHp_L5_radius HpHp_L5_covariance HpHp_L5_pcc HpHp_L3_weight HpHp_L3_mean HpHp_L3_std HpHp_L3_magnitude HpHp_L3_radius HpHp_L3_covariance HpHp_L3_pcc HpHp_L1_weight HpHp_L1_mean HpHp_L1_std HpHp_L1_magnitude HpHp_L1_radius HpHp_L1_covariance HpHp_L1_pcc HpHp_L0.1_weight HpHp_L0.1_mean HpHp_L0.1_std HpHp_L0.1_magnitude HpHp_L0.1_radius HpHp_L0.1_covariance HpHp_L0.1_pcc HpHp_L0.01_weight HpHp_L0.01_mean HpHp_L0.01_std HpHp_L0.01_magnitude HpHp_L0.01_radius HpHp_L0.01_covariance HpHp_L0.01_pcc target 01.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+00 0.0 0.0 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 11.996585566.05.820766e-111.997950566.05.820766e-111.999316566.00.000000e+001.999932566.01.746230e-101.999993566.01.164153e-101.996585566.05.820766e-111.997950566.05.820766e-111.999316566.00.000000e+001.999932566.01.746230e-101.999993566.01.164153e-101.996585566.00.000008566.05.820766e-110.00.01.997950566.00.000008566.05.820766e-110.00.01.999316566.00.000000566.00.000000e+000.00.01.999932566.00.000013566.01.746230e-100.00.01.999993566.00.000011566.01.164153e-10 0.0 0.0 1.996585 7.525396e+08 5.682564e+17 1.997950 7.530553e+08 5.682575e+17 1.999316 7.535711e+08 5.682580e+17 1.999932 7.538032e+08 5.682581e+17 1.999993 7.538264e+08 5.682581e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 22.958989566.00.000000e+002.975291566.05.820766e-112.991729566.05.820766e-112.999171566.00.000000e+002.999917566.05.820766e-112.958989566.00.000000e+002.975291566.05.820766e-112.991729566.05.820766e-112.999171566.00.000000e+002.999917566.05.820766e-112.958989566.00.000000566.00.000000e+000.00.02.975291566.00.000008566.05.820766e-110.00.02.991729566.00.000008566.05.820766e-110.00.02.999171566.00.000000566.00.000000e+000.00.02.999917566.00.000008566.05.820766e-11 0.0 0.0 2.958989 4.982164e+08 5.029203e+17 2.975291 4.999522e+08 5.038047e+17 2.991729 5.016863e+08 5.046822e+17 2.999171 5.024660e+08 5.050748e+17 2.999917 5.025440e+08 5.051139e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 33.958979566.00.000000e+003.975285566.00.000000e+003.991727566.01.164153e-103.999171566.00.000000e+003.999917566.01.164153e-103.958979566.00.000000e+003.975285566.00.000000e+003.991727566.01.164153e-103.999171566.00.000000e+003.999917566.01.164153e-103.958979566.00.000000566.00.000000e+000.00.03.975285566.00.000000566.00.000000e+000.00.03.991727566.00.000011566.01.164153e-100.00.03.999171566.00.000000566.00.000000e+000.00.03.999917566.00.000011566.01.164153e-10 0.0 0.0 3.958979 3.723717e+08 4.227485e+17 3.975285 3.741871e+08 4.241301e+17 3.991727 3.760048e+08 4.255070e+17 3.999171 3.768235e+08 4.261250e+17 3.999917 3.769054e+08 4.261867e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 44.914189566.01.164153e-104.948239566.05.820766e-114.982654566.05.820766e-114.998261566.00.000000e+004.999826566.01.164153e-104.914189566.01.164153e-104.948239566.05.820766e-114.982654566.05.820766e-114.998261566.00.000000e+004.999826566.01.164153e-104.914189566.00.000011566.01.164153e-100.00.04.948239566.00.000008566.05.820766e-110.00.04.982654566.00.000008566.05.820766e-110.00.04.998261566.00.000000566.00.000000e+000.00.04.999826566.00.000011566.01.164153e-10 0.0 0.0 4.914189 2.965969e+08 3.591969e+17 4.948239 2.985668e+08 3.609945e+17 4.982654 3.005420e+08 3.627891e+17 4.998261 3.014326e+08 3.635956e+17 4.999826 3.015217e+08 3.636762e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack In [44]:
gafgyt_df_list = []
In [45]:
for file in glob.glob("Dataset/gafgyt/*.csv"):
    tmp_df = pd.read_csv(file, nrows=rows)
    tmp_df["target"] = "gafgyt-"+ os.path.splitext(os.path.basename(file))[0]
    gafgyt_df_list.append(tmp_df)
In [46]:
gafgyt_df_list[0].head()
Out[46]:
MI_dir_L5_weightMI_dir_L5_meanMI_dir_L5_varianceMI_dir_L3_weightMI_dir_L3_meanMI_dir_L3_varianceMI_dir_L1_weightMI_dir_L1_meanMI_dir_L1_varianceMI_dir_L0.1_weightMI_dir_L0.1_meanMI_dir_L0.1_varianceMI_dir_L0.01_weightMI_dir_L0.01_meanMI_dir_L0.01_varianceH_L5_weightH_L5_meanH_L5_varianceH_L3_weightH_L3_meanH_L3_varianceH_L1_weightH_L1_meanH_L1_varianceH_L0.1_weightH_L0.1_meanH_L0.1_varianceH_L0.01_weightH_L0.01_meanH_L0.01_varianceHH_L5_weightHH_L5_meanHH_L5_stdHH_L5_magnitudeHH_L5_radiusHH_L5_covarianceHH_L5_pccHH_L3_weightHH_L3_meanHH_L3_stdHH_L3_magnitudeHH_L3_radiusHH_L3_covarianceHH_L3_pccHH_L1_weightHH_L1_meanHH_L1_stdHH_L1_magnitudeHH_L1_radiusHH_L1_covarianceHH_L1_pccHH_L0.1_weightHH_L0.1_meanHH_L0.1_stdHH_L0.1_magnitudeHH_L0.1_radiusHH_L0.1_covarianceHH_L0.1_pccHH_L0.01_weightHH_L0.01_meanHH_L0.01_stdHH_L0.01_magnitudeHH_L0.01_radius HH_L0.01_covariance HH_L0.01_pcc HH_jit_L5_weight HH_jit_L5_mean HH_jit_L5_variance HH_jit_L3_weight HH_jit_L3_mean HH_jit_L3_variance HH_jit_L1_weight HH_jit_L1_mean HH_jit_L1_variance HH_jit_L0.1_weight HH_jit_L0.1_mean HH_jit_L0.1_variance HH_jit_L0.01_weight HH_jit_L0.01_mean HH_jit_L0.01_variance HpHp_L5_weight HpHp_L5_mean HpHp_L5_std HpHp_L5_magnitude HpHp_L5_radius HpHp_L5_covariance HpHp_L5_pcc HpHp_L3_weight HpHp_L3_mean HpHp_L3_std HpHp_L3_magnitude HpHp_L3_radius HpHp_L3_covariance HpHp_L3_pcc HpHp_L1_weight HpHp_L1_mean HpHp_L1_std HpHp_L1_magnitude HpHp_L1_radius HpHp_L1_covariance HpHp_L1_pcc HpHp_L0.1_weight HpHp_L0.1_mean HpHp_L0.1_std HpHp_L0.1_magnitude HpHp_L0.1_radius HpHp_L0.1_covariance HpHp_L0.1_pcc HpHp_L0.01_weight HpHp_L0.01_mean HpHp_L0.01_std HpHp_L0.01_magnitude HpHp_L0.01_radius HpHp_L0.01_covariance HpHp_L0.01_pcc target 01.00000098.0000000.000000e+001.00000098.0000000.0000001.00000098.0000000.000000e+001.00000098.0000000.000000e+001.00000098.0000000.0000001.00000098.0000000.000000e+001.00000098.0000000.0000001.00000098.0000000.000000e+001.00000098.0000000.000000e+001.00000098.0000000.0000001.00000098.00.00000098.0000000.000000e+000.00.01.00000098.00.098.0000000.00.00.01.00000098.00.00000098.0000000.000000e+000.00.01.0000098.00.000000e+0098.0000000.000000e+000.00.01.00000098.00.00000098.0000000.000000e+00 0.0 0.0 1.000000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.00000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.000 98.0 0.000000 98.000000 0.000000e+00 0.0 0.0 1.00000 98.0 0.0 98.000000 0.0 0.0 0.0 1.000000 98.0 0.000000 98.000000 0.000000e+00 0.0 0.0 1.00000 98.0 0.000000 98.000000 0.000000e+00 0.0 0.0 1.000000 98.0 0.000000 98.000000 0.000000e+00 0.0 0.0 gafgyt-combo 11.02900098.0000001.818989e-121.11952098.0000000.0000001.49258398.0000003.637979e-121.93164098.0000001.818989e-121.99294498.0000000.0000001.02900098.0000001.818989e-121.11952098.0000000.0000001.49258398.0000003.637979e-121.93164098.0000001.818989e-121.99294498.0000000.0000001.02900098.00.000001138.5929291.818989e-120.00.01.11952098.00.0138.5929290.00.00.01.49258398.00.000002138.5929293.637979e-120.00.01.9316498.01.348699e-06138.5929291.818989e-120.00.01.99294498.00.000001138.5929291.818989e-12 0.0 0.0 1.029000 4.244084e+07 6.211104e+16 1.119520 1.607711e+08 2.162601e+17 1.492583 4.969829e+08 5.014217e+17 1.93164 7.263102e+08 5.662344e+17 1.992944 7.502914e+08 5.669374e+17 1.029 98.0 0.000001 138.592929 1.818989e-12 0.0 0.0 1.11952 98.0 0.0 138.592929 0.0 0.0 0.0 1.492583 98.0 0.000002 138.592929 3.637979e-12 0.0 0.0 1.93164 98.0 0.000001 138.592929 1.818989e-12 0.0 0.0 1.992944 98.0 0.000001 138.592929 1.818989e-12 0.0 0.0 gafgyt-combo 21.50415676.7256122.281808e+021.72966279.499272249.7463572.29410284.0511882.517926e+022.90427386.9817502.311822e+022.99010287.298025227.9309281.50415676.7256122.281808e+021.72966279.499272249.7463572.29410284.0511882.517926e+022.90427386.9817502.311822e+022.99010287.298025227.9309281.00000066.00.000000114.8564320.000000e+000.00.01.00000066.00.0114.8564320.00.00.01.00000066.00.000000114.8564320.000000e+000.00.01.0000066.00.000000e+00114.8564320.000000e+000.00.01.00000066.00.000000114.8564320.000000e+00 0.0 0.0 1.000000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.00000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.000 66.0 0.000000 114.856432 0.000000e+00 0.0 0.0 1.00000 66.0 0.0 114.856432 0.0 0.0 0.0 1.000000 66.0 0.000000 114.856432 0.000000e+00 0.0 0.0 1.00000 66.0 0.000000 114.856432 0.000000e+00 0.0 0.0 1.000000 66.0 0.000000 114.856432 0.000000e+00 0.0 0.0 gafgyt-combo 32.46008775.6176791.372200e+022.69907577.461807164.2693313.28049980.9872671.964467e+023.90254683.6552682.040614e+023.98992583.965124204.0170782.46008775.6176791.372200e+022.69907577.461807164.2693313.28049980.9872671.964467e+023.90254683.6552682.040614e+023.98992583.965124204.0170781.00000074.00.00000074.0000000.000000e+000.00.01.00000074.00.074.0000000.00.00.01.00000074.00.00000074.0000000.000000e+000.00.01.0000074.00.000000e+0074.0000000.000000e+000.00.01.00000074.00.00000074.0000000.000000e+00 0.0 0.0 1.000000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.00000 1.505914e+09 0.000000e+00 1.000000 1.505914e+09 0.000000e+00 1.000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 1.00000 74.0 0.0 74.000000 0.0 0.0 0.0 1.000000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 1.00000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 1.000000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 gafgyt-combo 43.46005575.1501499.809937e+013.69905476.525944122.2247984.28049079.3549151.592943e+024.90254581.6858281.775746e+024.98992481.968075179.0439093.46005575.1501499.809937e+013.69905476.525944122.2247984.28049079.3549151.592943e+024.90254581.6858281.775746e+024.98992481.968075179.0439091.99998774.00.00000074.0000000.000000e+000.00.01.99999274.00.074.0000000.00.00.01.99999774.00.00000174.0000001.818989e-120.00.02.0000074.09.536743e-0774.0000009.094947e-130.00.02.00000074.00.00000174.0000001.818989e-12 0.0 0.0 1.999987 7.529522e+08 5.669445e+17 1.999992 7.529542e+08 5.669445e+17 1.999997 7.529562e+08 5.669445e+17 2.00000 7.529571e+08 5.669445e+17 2.000000 7.529572e+08 5.669445e+17 1.000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 1.00000 74.0 0.0 74.000000 0.0 0.0 0.0 1.000000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 1.00000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 1.000000 74.0 0.000000 74.000000 0.000000e+00 0.0 0.0 gafgyt-combo In [47]:
benign_df = pd.read_csv("Dataset/benign_traffic.csv", nrows=rows)
benign_df["target"] = "benign"
In [48]:
gafgyt_df_list.append(benign_df)
In [49]:
df_list = mirai_df_list + gafgyt_df_list
In [50]:
df = pd.concat(df_list)
In [51]:
df.head()
Out[51]:
MI_dir_L5_weightMI_dir_L5_meanMI_dir_L5_varianceMI_dir_L3_weightMI_dir_L3_meanMI_dir_L3_varianceMI_dir_L1_weightMI_dir_L1_meanMI_dir_L1_varianceMI_dir_L0.1_weightMI_dir_L0.1_meanMI_dir_L0.1_varianceMI_dir_L0.01_weightMI_dir_L0.01_meanMI_dir_L0.01_varianceH_L5_weightH_L5_meanH_L5_varianceH_L3_weightH_L3_meanH_L3_varianceH_L1_weightH_L1_meanH_L1_varianceH_L0.1_weightH_L0.1_meanH_L0.1_varianceH_L0.01_weightH_L0.01_meanH_L0.01_varianceHH_L5_weightHH_L5_meanHH_L5_stdHH_L5_magnitudeHH_L5_radiusHH_L5_covarianceHH_L5_pccHH_L3_weightHH_L3_meanHH_L3_stdHH_L3_magnitudeHH_L3_radiusHH_L3_covarianceHH_L3_pccHH_L1_weightHH_L1_meanHH_L1_stdHH_L1_magnitudeHH_L1_radiusHH_L1_covarianceHH_L1_pccHH_L0.1_weightHH_L0.1_meanHH_L0.1_stdHH_L0.1_magnitudeHH_L0.1_radiusHH_L0.1_covarianceHH_L0.1_pccHH_L0.01_weightHH_L0.01_meanHH_L0.01_stdHH_L0.01_magnitudeHH_L0.01_radius HH_L0.01_covariance HH_L0.01_pcc HH_jit_L5_weight HH_jit_L5_mean HH_jit_L5_variance HH_jit_L3_weight HH_jit_L3_mean HH_jit_L3_variance HH_jit_L1_weight HH_jit_L1_mean HH_jit_L1_variance HH_jit_L0.1_weight HH_jit_L0.1_mean HH_jit_L0.1_variance HH_jit_L0.01_weight HH_jit_L0.01_mean HH_jit_L0.01_variance HpHp_L5_weight HpHp_L5_mean HpHp_L5_std HpHp_L5_magnitude HpHp_L5_radius HpHp_L5_covariance HpHp_L5_pcc HpHp_L3_weight HpHp_L3_mean HpHp_L3_std HpHp_L3_magnitude HpHp_L3_radius HpHp_L3_covariance HpHp_L3_pcc HpHp_L1_weight HpHp_L1_mean HpHp_L1_std HpHp_L1_magnitude HpHp_L1_radius HpHp_L1_covariance HpHp_L1_pcc HpHp_L0.1_weight HpHp_L0.1_mean HpHp_L0.1_std HpHp_L0.1_magnitude HpHp_L0.1_radius HpHp_L0.1_covariance HpHp_L0.1_pcc HpHp_L0.01_weight HpHp_L0.01_mean HpHp_L0.01_std HpHp_L0.01_magnitude HpHp_L0.01_radius HpHp_L0.01_covariance HpHp_L0.01_pcc target 01.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000e+001.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+000.00.01.000000566.00.000000566.00.000000e+00 0.0 0.0 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.000000 1.507658e+09 0.000000e+00 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 11.996585566.05.820766e-111.997950566.05.820766e-111.999316566.00.000000e+001.999932566.01.746230e-101.999993566.01.164153e-101.996585566.05.820766e-111.997950566.05.820766e-111.999316566.00.000000e+001.999932566.01.746230e-101.999993566.01.164153e-101.996585566.00.000008566.05.820766e-110.00.01.997950566.00.000008566.05.820766e-110.00.01.999316566.00.000000566.00.000000e+000.00.01.999932566.00.000013566.01.746230e-100.00.01.999993566.00.000011566.01.164153e-10 0.0 0.0 1.996585 7.525396e+08 5.682564e+17 1.997950 7.530553e+08 5.682575e+17 1.999316 7.535711e+08 5.682580e+17 1.999932 7.538032e+08 5.682581e+17 1.999993 7.538264e+08 5.682581e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 22.958989566.00.000000e+002.975291566.05.820766e-112.991729566.05.820766e-112.999171566.00.000000e+002.999917566.05.820766e-112.958989566.00.000000e+002.975291566.05.820766e-112.991729566.05.820766e-112.999171566.00.000000e+002.999917566.05.820766e-112.958989566.00.000000566.00.000000e+000.00.02.975291566.00.000008566.05.820766e-110.00.02.991729566.00.000008566.05.820766e-110.00.02.999171566.00.000000566.00.000000e+000.00.02.999917566.00.000008566.05.820766e-11 0.0 0.0 2.958989 4.982164e+08 5.029203e+17 2.975291 4.999522e+08 5.038047e+17 2.991729 5.016863e+08 5.046822e+17 2.999171 5.024660e+08 5.050748e+17 2.999917 5.025440e+08 5.051139e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 33.958979566.00.000000e+003.975285566.00.000000e+003.991727566.01.164153e-103.999171566.00.000000e+003.999917566.01.164153e-103.958979566.00.000000e+003.975285566.00.000000e+003.991727566.01.164153e-103.999171566.00.000000e+003.999917566.01.164153e-103.958979566.00.000000566.00.000000e+000.00.03.975285566.00.000000566.00.000000e+000.00.03.991727566.00.000011566.01.164153e-100.00.03.999171566.00.000000566.00.000000e+000.00.03.999917566.00.000011566.01.164153e-10 0.0 0.0 3.958979 3.723717e+08 4.227485e+17 3.975285 3.741871e+08 4.241301e+17 3.991727 3.760048e+08 4.255070e+17 3.999171 3.768235e+08 4.261250e+17 3.999917 3.769054e+08 4.261867e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack 44.914189566.01.164153e-104.948239566.05.820766e-114.982654566.05.820766e-114.998261566.00.000000e+004.999826566.01.164153e-104.914189566.01.164153e-104.948239566.05.820766e-114.982654566.05.820766e-114.998261566.00.000000e+004.999826566.01.164153e-104.914189566.00.000011566.01.164153e-100.00.04.948239566.00.000008566.05.820766e-110.00.04.982654566.00.000008566.05.820766e-110.00.04.998261566.00.000000566.00.000000e+000.00.04.999826566.00.000011566.01.164153e-10 0.0 0.0 4.914189 2.965969e+08 3.591969e+17 4.948239 2.985668e+08 3.609945e+17 4.982654 3.005420e+08 3.627891e+17 4.998261 3.014326e+08 3.635956e+17 4.999826 3.015217e+08 3.636762e+17 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 1.0 566.0 0.0 566.0 0.0 0.0 0.0 mirai-ack In [52]:
df.info()
<class 'pandas.core.frame.DataFrame'>
Index: 110000 entries, 0 to 9999
Columns: 116 entries, MI_dir_L5_weight to target
dtypes: float64(115), object(1)
memory usage: 98.2+ MB
In [53]:
df.describe()
Out[53]:
MI_dir_L5_weightMI_dir_L5_meanMI_dir_L5_varianceMI_dir_L3_weightMI_dir_L3_meanMI_dir_L3_varianceMI_dir_L1_weightMI_dir_L1_meanMI_dir_L1_varianceMI_dir_L0.1_weightMI_dir_L0.1_meanMI_dir_L0.1_varianceMI_dir_L0.01_weightMI_dir_L0.01_meanMI_dir_L0.01_varianceH_L5_weightH_L5_meanH_L5_varianceH_L3_weightH_L3_meanH_L3_varianceH_L1_weightH_L1_meanH_L1_varianceH_L0.1_weightH_L0.1_meanH_L0.1_varianceH_L0.01_weightH_L0.01_meanH_L0.01_varianceHH_L5_weightHH_L5_meanHH_L5_stdHH_L5_magnitudeHH_L5_radiusHH_L5_covarianceHH_L5_pccHH_L3_weightHH_L3_meanHH_L3_stdHH_L3_magnitudeHH_L3_radiusHH_L3_covarianceHH_L3_pccHH_L1_weightHH_L1_meanHH_L1_stdHH_L1_magnitudeHH_L1_radiusHH_L1_covarianceHH_L1_pccHH_L0.1_weightHH_L0.1_meanHH_L0.1_stdHH_L0.1_magnitudeHH_L0.1_radiusHH_L0.1_covarianceHH_L0.1_pccHH_L0.01_weightHH_L0.01_meanHH_L0.01_stdHH_L0.01_magnitudeHH_L0.01_radius HH_L0.01_covariance HH_L0.01_pcc HH_jit_L5_weight HH_jit_L5_mean HH_jit_L5_variance HH_jit_L3_weight HH_jit_L3_mean HH_jit_L3_variance HH_jit_L1_weight HH_jit_L1_mean HH_jit_L1_variance HH_jit_L0.1_weight HH_jit_L0.1_mean HH_jit_L0.1_variance HH_jit_L0.01_weight HH_jit_L0.01_mean HH_jit_L0.01_variance HpHp_L5_weight HpHp_L5_mean HpHp_L5_std HpHp_L5_magnitude HpHp_L5_radius HpHp_L5_covariance HpHp_L5_pcc HpHp_L3_weight HpHp_L3_mean HpHp_L3_std HpHp_L3_magnitude HpHp_L3_radius HpHp_L3_covariance HpHp_L3_pcc HpHp_L1_weight HpHp_L1_mean HpHp_L1_std HpHp_L1_magnitude HpHp_L1_radius HpHp_L1_covariance HpHp_L1_pcc HpHp_L0.1_weight HpHp_L0.1_mean HpHp_L0.1_std HpHp_L0.1_magnitude HpHp_L0.1_radius HpHp_L0.1_covariance HpHp_L0.1_pcc HpHp_L0.01_weight HpHp_L0.01_mean HpHp_L0.01_std HpHp_L0.01_magnitude HpHp_L0.01_radius HpHp_L0.01_covariance HpHp_L0.01_pcc count110000.000000110000.0000001.100000e+05110000.000000110000.0000001.100000e+05110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.0000001.100000e+05110000.000000110000.0000001.100000e+05110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.000000110000.0000001.100000e+05110000.000000110000.000000110000.000000110000.000000110000.000000110000.0000001.100000e+05110000.000000110000.000000110000.000000110000.0000001.100000e+05110000.0000001.100000e+05110000.000000110000.000000110000.000000110000.0000001.100000e+05110000.0000001.100000e+05110000.000000110000.000000110000.000000110000.000000110000.000000110000.0000001.100000e+05 110000.000000 110000.000000 110000.000000 1.100000e+05 1.100000e+05 110000.000000 1.100000e+05 1.100000e+05 110000.000000 1.100000e+05 1.100000e+05 110000.000000 1.100000e+05 1.100000e+05 110000.000000 1.100000e+05 1.100000e+05 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 1.100000e+05 1.100000e+05 110000.000000 110000.000000 110000.000000 110000.000000 110000.000000 1.100000e+05 1.100000e+05 mean90.516804150.7257111.323286e+04143.049871151.4781251.474774e+04403.674936152.41486515768.3516092914.001854152.71415815928.3848316743.832189152.37696515924.73643290.516804150.7257111.323286e+04143.049871151.4781251.474774e+04403.674937152.41485915768.3516882914.001883152.71401815928.3857236743.832268152.37676415924.73704849.839791152.0110920.132021159.8035013.414243e+010.0522520.00007681.403775152.0067550.186965159.8029753.607599e+010.0708510.000212234.798829152.0119744.229231e-01159.8163084.091873e+010.1452030.0010491466.875958152.0954201.451430e+00159.9043841.193269e+029.3051740.0058402726.929858152.0306601.833534159.8206691.818674e+02 26.474414 0.010212 49.839791 6.660269e+08 2.205910e+14 81.403775 6.661215e+08 3.505572e+14 234.798829 6.703483e+08 5.993848e+15 1466.875958 6.929222e+08 2.578031e+16 2726.929858 6.985278e+08 2.877678e+16 4.161579 152.026324 0.010044 155.967607 1.288912 0.044769 0.000006 6.113788 152.025575 0.010754 155.966824 1.471644 0.054842 0.000008 15.760525 152.024874 0.013320 155.964218 1.785494 0.078245 0.000014 134.349513 152.041837 0.051048 155.926754 49.138696 7.667726e+00 1.761195e-04 337.486779 152.047478 0.063018 155.897986 77.620206 2.016201e+01 4.232043e-04 std68.164340142.1004422.278691e+04109.090902137.5020612.440576e+04316.223089134.72244525512.5637582345.120262134.29346925625.8334165459.810151134.08469825670.41651468.164340142.1004422.278691e+04109.090902137.5020612.440576e+04316.223088134.72243625512.5637092345.120226134.29326025625.8328635459.810054134.08440025670.41613265.768487183.8532473.482893182.9583173.331245e+0312.0997570.003750107.612503183.8407643.782331182.9565523.285212e+0313.8698070.006814311.012432183.8150354.274199e+00182.9423603.223009e+0316.5947330.0197161894.118754183.7186226.194772e+00182.6073624.699331e+03518.9806060.0543223650.416970183.7510686.604923182.3765267.099022e+03 1316.701828 0.064828 65.768487 7.481714e+08 9.520988e+15 107.612503 7.480956e+08 9.977099e+15 311.012432 7.447952e+08 3.645123e+16 1894.118754 7.335894e+08 1.091016e+17 3650.416970 7.319827e+08 1.189094e+17 14.313876 183.908970 1.012180 184.518476 188.599618 11.914098 0.001750 22.659881 183.908173 1.067976 184.517288 201.255873 13.581245 0.002044 64.128860 183.907172 1.119633 184.515324 214.882962 16.278067 0.002729 576.782043 183.902725 2.623019 184.273728 2791.756019 4.967791e+02 1.078563e-02 1478.276186 183.903129 2.953697 184.111122 4456.762244 1.268258e+03 1.877671e-02 min1.00000060.0000000.000000e+001.00000060.0000000.000000e+001.00000060.0000000.0000001.00000060.0000000.0000001.00000060.0000000.0000001.00000060.0000000.000000e+001.00000060.0000000.000000e+001.00000060.0000000.0000001.00000060.0000000.0000001.00000060.0000000.0000001.00000060.0000000.00000060.0000000.000000e+00-259.919840-0.1692021.00000060.0000000.00000060.0000000.000000e+00-182.592006-0.1881801.00000060.0000000.000000e+0060.0000000.000000e+00-497.515497-0.4590591.00000060.0000000.000000e+0060.0000000.000000e+00-21023.964871-0.5216671.00000060.0000000.00000060.0000000.000000e+00 -1875.753464 -0.572367 1.000000 1.074374e-03 0.000000e+00 1.000000 1.251909e-03 0.000000e+00 1.000000 3.120875e-03 0.000000e+00 1.000000 5.299690e+00 0.000000e+00 1.000000 9.590803e+00 0.000000e+00 1.000000 60.000000 0.000000 60.000000 0.000000 -259.919840 -0.115918 1.000000 60.000000 0.000000 60.000000 0.000000 -182.592006 -0.068022 1.000000 60.000000 0.000000 60.000000 0.000000 -495.192466 -0.112410 1.000000 60.000000 0.000000 60.000000 0.000000 -3.182929e-28 -1.590000e-16 1.000000 60.000000 0.000000 60.000000 0.000000 -2.110595e-27 -6.972299e-16 25%2.99988360.0000036.366463e-123.07419060.0010077.516427e-073.91425060.0206420.0499715.58541460.0238950.90834431.69269360.0223410.7763032.99988360.0000036.366463e-123.07419060.0010077.565479e-073.91425060.0206420.0500315.58541460.0238950.90850431.69269360.0223410.7763281.00000060.0000000.00000060.0000000.000000e+000.0000000.0000001.00000060.0000000.00000060.0000000.000000e+000.0000000.0000001.00000060.0000000.000000e+0060.0000000.000000e+000.0000000.0000001.00000060.0000000.000000e+0060.0000000.000000e+000.0000000.0000001.00000060.0000000.00000060.0000000.000000e+00 0.000000 0.000000 1.000000 3.583776e-03 0.000000e+00 1.000000 4.155401e-03 0.000000e+00 1.000000 1.356516e+01 0.000000e+00 1.000000 6.690028e+04 0.000000e+00 1.000000 1.640511e+05 0.000000e+00 1.000000 60.000000 0.000000 60.000000 0.000000 0.000000 0.000000 1.000000 60.000000 0.000000 60.000000 0.000000 0.000000 0.000000 1.000000 60.000000 0.000000 60.000000 0.000000 0.000000 0.000000 1.000000 60.000000 0.000000 60.000000 0.000000 0.000000e+00 0.000000e+00 1.000000 60.000000 0.000000 60.000000 0.000000 0.000000e+00 0.000000e+00 50%105.50470174.0325481.709421e+00173.32381174.0475812.163125e+00509.38219674.05726417.5440942520.54597074.11830243.9714557005.02923774.11864343.944130105.50470174.0325481.709421e+00173.32381174.0475812.163125e+00509.38219674.05726417.5440942520.54597074.11830243.9715397005.02923774.11864343.9441771.00000074.0000000.00000074.0000000.000000e+000.0000000.0000001.00000074.0000000.00000074.0000000.000000e+000.0000000.0000001.00000073.9997479.536743e-0774.0000009.094947e-130.0000000.0000001.29184673.9967149.536743e-0774.0000001.818989e-120.0000000.0000004.13178173.9965340.00000174.0000001.820000e-12 0.000000 0.000000 1.000000 3.908631e+01 0.000000e+00 1.000000 1.208891e+03 0.000000e+00 1.000000 8.903386e+05 1.170000e-06 1.291846 2.115655e+06 2.611118e+01 4.131781 3.206240e+06 9.093492e+01 1.000000 74.000000 0.000000 74.000000 0.000000 0.000000 0.000000 1.000000 74.000000 0.000000 74.000000 0.000000 0.000000 0.000000 1.000000 74.000000 0.000000 74.000000 0.000000 0.000000 0.000000 1.000000 74.000000 0.000000 74.000000 0.000000 0.000000e+00 0.000000e+00 1.000000 74.000000 0.000000 74.000000 0.000000 0.000000e+00 0.000000e+00 75%150.872531218.2504502.248560e+04240.402077267.4688693.721092e+04665.826883315.40682751135.7789695254.963508335.42317154986.58226411474.326434336.22719055251.162866150.872531218.2504502.248560e+04240.402077267.4688693.721092e+04665.826883315.40682751135.7789695254.963508335.42317154986.58226411474.326434336.22719055251.16286691.46377374.0000000.000004113.9769455.820766e-110.0000000.000000148.17224474.0000000.000013112.0219042.328306e-100.0000000.000000430.20719274.0000005.722663e-03107.0349024.261974e-050.0000000.0000003435.96772175.0247182.731057e-01106.5142947.509882e-020.0000000.0000006194.45510176.2045090.273765107.6639587.533794e-02 0.000000 0.000000 91.463773 1.505914e+09 1.699846e-04 148.172244 1.505914e+09 2.254492e-01 430.207192 1.505914e+09 8.205879e+09 3435.967721 1.505914e+09 1.399161e+14 6194.455101 1.505914e+09 2.872196e+14 1.000000 74.000000 0.000000 84.852814 0.000000 0.000000 0.000000 1.000000 74.000000 0.000000 84.852814 0.000000 0.000000 0.000000 1.000000 74.000000 0.000000 84.852814 0.000000 0.000000 0.000000 1.000000 74.000000 0.000000 84.852814 0.000000 0.000000e+00 0.000000e+00 1.000000 74.000000 0.000000 84.852814 0.000000 0.000000e+00 0.000000e+00 max289.940269829.6886151.738886e+05403.474557566.0000001.731848e+05952.290732566.00000072079.4064766160.257527566.00000060443.15209718012.013676566.00000060319.472596289.940269829.6886151.738886e+05403.474557566.0000001.731848e+05952.290732566.00000072079.4064766160.257527566.00000060443.15209718012.013676566.00000060319.472596289.687972909.000000420.2783481456.0273154.850655e+053591.8960370.491443402.996925909.000000422.9720991456.0271364.842003e+053898.8818010.566967950.445201909.0000004.243298e+021455.8700144.744260e+054245.4771010.8110505996.722086909.0000004.244983e+021348.3446744.771964e+0557695.8382481.19184711740.678273909.000000424.4999831247.6668484.835736e+05 134513.831508 1.528392 289.687972 1.507659e+09 5.682564e+17 402.996925 1.507659e+09 5.682575e+17 950.445201 1.507659e+09 5.682580e+17 5996.722086 1.507659e+09 5.682591e+17 11740.678273 1.507659e+09 5.682591e+17 156.972932 909.000000 203.281372 1456.027315 41323.316283 3530.229418 0.483006 205.772479 909.000000 201.210480 1456.027136 40485.657329 3812.913778 0.554466 406.016789 909.000000 199.458107 1455.870014 39783.536270 4185.214607 0.680974 2786.340597 909.000000 202.507990 1348.344674 167094.050789 5.769584e+04 9.338040e-01 8682.051611 909.000000 203.212189 1257.118239 273875.922655 1.345138e+05 1.528392e+00 In [54]:
ds_shape = df.shape
In [55]:
display(HTML("<h6 class='messagebox messagelightgreen'>No of Rows Available in Dataset <b>{0}</b></h6>".format(ds_shape[0])))
display(HTML("<h6 class='messagebox messagelightgreen'>No of Columns Available in Dataset <b>{0}</b></h6>".format(ds_shape[1])))
No of Rows Available in Dataset 110000
No of Columns Available in Dataset 116
In [56]:
target_counts = df["target"].value_counts()
In [57]:
target_counts
Out[57]:
target
mirai-ack         10000
mirai-scan        10000
mirai-syn         10000
mirai-udp         10000
mirai-udpplain    10000
gafgyt-combo      10000
gafgyt-junk       10000
gafgyt-scan       10000
gafgyt-tcp        10000
gafgyt-udp        10000
benign            10000
Name: count, dtype: int64
In [58]:
def display_bar_chart(data, xvalues, yvalues, graph_title, xtitle, ytitle):
    fig = px.bar(data, x=xvalues, y=yvalues, color=xvalues)
    fig.update_layout(title=graph_title, xaxis_title=xtitle, yaxis_title=ytitle)
    fig.show()
In [59]:
display_bar_chart(target_counts, target_counts.index, target_counts.values, "Target wise count", "IoT BoT Name", "No of Samples")
In [60]:
df.isnull().sum().sort_values()
Out[60]:
MI_dir_L5_weight     0
HpHp_L5_radius       0
HpHp_L5_magnitude    0
HpHp_L5_std          0
HpHp_L5_mean         0
                    ..
HH_L5_std            0
HH_L5_mean           0
HH_L5_weight         0
HH_L3_covariance     0
target               0
Length: 116, dtype: int64
In [61]:
display(HTML("<h6 class='messagebox messagebrown'><b>No</b> Missing Values are Available in Dataset</h6>"))
No Missing Values are Available in Dataset
In [62]:
duplicated_rows = df.duplicated().sum()
In [63]:
display(HTML("<h6 class='messagebox messagebrown'>There are {0} Duplicated Rows Available in Dataset</h6>".format(duplicated_rows)))
There are 1343 Duplicated Rows Available in Dataset
In [64]:
df.drop_duplicates(inplace=True)
In [65]:
duplicated_rows = df.duplicated().sum()
In [66]:
display(HTML("<h6 class='messagebox messagebrown'>There are {0} Duplicated Rows Available in Dataset</h6>".format(duplicated_rows)))
There are 0 Duplicated Rows Available in Dataset
In [67]:
counts = df.nunique()
In [68]:
counts
Out[68]:
MI_dir_L5_weight         86069
MI_dir_L5_mean           79492
MI_dir_L5_variance       80995
MI_dir_L3_weight         87715
MI_dir_L3_mean           84051
                         ...  
HpHp_L0.01_magnitude       362
HpHp_L0.01_radius          697
HpHp_L0.01_covariance     4448
HpHp_L0.01_pcc            3456
target                      11
Length: 116, dtype: int64
In [69]:
df["target"].replace(regex=r'^mirai-[a-z]*', value='mirai', inplace=True)
In [70]:
df["target"].replace(regex=r'^gafgyt-[a-z]*', value='gafgyt', inplace=True)
In [71]:
target_counts = df["target"].value_counts()
In [72]:
display_bar_chart(target_counts, target_counts.index, target_counts.values, "Target wise count", "IoT BoT Name", "No of Samples")
In [73]:
print(df.columns.tolist())
['MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance', 'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance', 'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance', 'MI_dir_L0.1_weight', 'MI_dir_L0.1_mean', 'MI_dir_L0.1_variance', 'MI_dir_L0.01_weight', 'MI_dir_L0.01_mean', 'MI_dir_L0.01_variance', 'H_L5_weight', 'H_L5_mean', 'H_L5_variance', 'H_L3_weight', 'H_L3_mean', 'H_L3_variance', 'H_L1_weight', 'H_L1_mean', 'H_L1_variance', 'H_L0.1_weight', 'H_L0.1_mean', 'H_L0.1_variance', 'H_L0.01_weight', 'H_L0.01_mean', 'H_L0.01_variance', 'HH_L5_weight', 'HH_L5_mean', 'HH_L5_std', 'HH_L5_magnitude', 'HH_L5_radius', 'HH_L5_covariance', 'HH_L5_pcc', 'HH_L3_weight', 'HH_L3_mean', 'HH_L3_std', 'HH_L3_magnitude', 'HH_L3_radius', 'HH_L3_covariance', 'HH_L3_pcc', 'HH_L1_weight', 'HH_L1_mean', 'HH_L1_std', 'HH_L1_magnitude', 'HH_L1_radius', 'HH_L1_covariance', 'HH_L1_pcc', 'HH_L0.1_weight', 'HH_L0.1_mean', 'HH_L0.1_std', 'HH_L0.1_magnitude', 'HH_L0.1_radius', 'HH_L0.1_covariance', 'HH_L0.1_pcc', 'HH_L0.01_weight', 'HH_L0.01_mean', 'HH_L0.01_std', 'HH_L0.01_magnitude', 'HH_L0.01_radius', 'HH_L0.01_covariance', 'HH_L0.01_pcc', 'HH_jit_L5_weight', 'HH_jit_L5_mean', 'HH_jit_L5_variance', 'HH_jit_L3_weight', 'HH_jit_L3_mean', 'HH_jit_L3_variance', 'HH_jit_L1_weight', 'HH_jit_L1_mean', 'HH_jit_L1_variance', 'HH_jit_L0.1_weight', 'HH_jit_L0.1_mean', 'HH_jit_L0.1_variance', 'HH_jit_L0.01_weight', 'HH_jit_L0.01_mean', 'HH_jit_L0.01_variance', 'HpHp_L5_weight', 'HpHp_L5_mean', 'HpHp_L5_std', 'HpHp_L5_magnitude', 'HpHp_L5_radius', 'HpHp_L5_covariance', 'HpHp_L5_pcc', 'HpHp_L3_weight', 'HpHp_L3_mean', 'HpHp_L3_std', 'HpHp_L3_magnitude', 'HpHp_L3_radius', 'HpHp_L3_covariance', 'HpHp_L3_pcc', 'HpHp_L1_weight', 'HpHp_L1_mean', 'HpHp_L1_std', 'HpHp_L1_magnitude', 'HpHp_L1_radius', 'HpHp_L1_covariance', 'HpHp_L1_pcc', 'HpHp_L0.1_weight', 'HpHp_L0.1_mean', 'HpHp_L0.1_std', 'HpHp_L0.1_magnitude', 'HpHp_L0.1_radius', 'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc', 'HpHp_L0.01_weight', 'HpHp_L0.01_mean', 'HpHp_L0.01_std', 'HpHp_L0.01_magnitude', 'HpHp_L0.01_radius', 'HpHp_L0.01_covariance', 'HpHp_L0.01_pcc', 'target']
In [74]:
px.scatter(df, x=df.MI_dir_L5_weight, y=df.MI_dir_L1_weight, color=df.target)
In [75]:
px.line(df, x=df.MI_dir_L5_weight, y=df.MI_dir_L1_weight, color=df.target)
In [76]:
df.columns.tolist()
Out[76]:
['MI_dir_L5_weight',
 'MI_dir_L5_mean',
 'MI_dir_L5_variance',
 'MI_dir_L3_weight',
 'MI_dir_L3_mean',
 'MI_dir_L3_variance',
 'MI_dir_L1_weight',
 'MI_dir_L1_mean',
 'MI_dir_L1_variance',
 'MI_dir_L0.1_weight',
 'MI_dir_L0.1_mean',
 'MI_dir_L0.1_variance',
 'MI_dir_L0.01_weight',
 'MI_dir_L0.01_mean',
 'MI_dir_L0.01_variance',
 'H_L5_weight',
 'H_L5_mean',
 'H_L5_variance',
 'H_L3_weight',
 'H_L3_mean',
 'H_L3_variance',
 'H_L1_weight',
 'H_L1_mean',
 'H_L1_variance',
 'H_L0.1_weight',
 'H_L0.1_mean',
 'H_L0.1_variance',
 'H_L0.01_weight',
 'H_L0.01_mean',
 'H_L0.01_variance',
 'HH_L5_weight',
 'HH_L5_mean',
 'HH_L5_std',
 'HH_L5_magnitude',
 'HH_L5_radius',
 'HH_L5_covariance',
 'HH_L5_pcc',
 'HH_L3_weight',
 'HH_L3_mean',
 'HH_L3_std',
 'HH_L3_magnitude',
 'HH_L3_radius',
 'HH_L3_covariance',
 'HH_L3_pcc',
 'HH_L1_weight',
 'HH_L1_mean',
 'HH_L1_std',
 'HH_L1_magnitude',
 'HH_L1_radius',
 'HH_L1_covariance',
 'HH_L1_pcc',
 'HH_L0.1_weight',
 'HH_L0.1_mean',
 'HH_L0.1_std',
 'HH_L0.1_magnitude',
 'HH_L0.1_radius',
 'HH_L0.1_covariance',
 'HH_L0.1_pcc',
 'HH_L0.01_weight',
 'HH_L0.01_mean',
 'HH_L0.01_std',
 'HH_L0.01_magnitude',
 'HH_L0.01_radius',
 'HH_L0.01_covariance',
 'HH_L0.01_pcc',
 'HH_jit_L5_weight',
 'HH_jit_L5_mean',
 'HH_jit_L5_variance',
 'HH_jit_L3_weight',
 'HH_jit_L3_mean',
 'HH_jit_L3_variance',
 'HH_jit_L1_weight',
 'HH_jit_L1_mean',
 'HH_jit_L1_variance',
 'HH_jit_L0.1_weight',
 'HH_jit_L0.1_mean',
 'HH_jit_L0.1_variance',
 'HH_jit_L0.01_weight',
 'HH_jit_L0.01_mean',
 'HH_jit_L0.01_variance',
 'HpHp_L5_weight',
 'HpHp_L5_mean',
 'HpHp_L5_std',
 'HpHp_L5_magnitude',
 'HpHp_L5_radius',
 'HpHp_L5_covariance',
 'HpHp_L5_pcc',
 'HpHp_L3_weight',
 'HpHp_L3_mean',
 'HpHp_L3_std',
 'HpHp_L3_magnitude',
 'HpHp_L3_radius',
 'HpHp_L3_covariance',
 'HpHp_L3_pcc',
 'HpHp_L1_weight',
 'HpHp_L1_mean',
 'HpHp_L1_std',
 'HpHp_L1_magnitude',
 'HpHp_L1_radius',
 'HpHp_L1_covariance',
 'HpHp_L1_pcc',
 'HpHp_L0.1_weight',
 'HpHp_L0.1_mean',
 'HpHp_L0.1_std',
 'HpHp_L0.1_magnitude',
 'HpHp_L0.1_radius',
 'HpHp_L0.1_covariance',
 'HpHp_L0.1_pcc',
 'HpHp_L0.01_weight',
 'HpHp_L0.01_mean',
 'HpHp_L0.01_std',
 'HpHp_L0.01_magnitude',
 'HpHp_L0.01_radius',
 'HpHp_L0.01_covariance',
 'HpHp_L0.01_pcc',
 'target']
In [77]:
def display_scatter_matrix(matrix_data, color_variable, symbol_variable):
    fig = px.scatter_matrix(df, dimensions=matrix_data, color=color_variable, symbol=symbol_variable, height=1200)
    fig.update_traces(diagonal_visible=False)
    fig.show()
In [78]:
l5_weight = [c for c in df.columns.tolist() if c.endswith("L5_weight")]
display_scatter_matrix(l5_weight, "target", "target")
In [79]:
matrix_data = [c for c in df.columns.tolist() if c.endswith("L5_mean")]
display_scatter_matrix(matrix_data, "target", "target")
In [80]:
matrix_data = [c for c in df.columns.tolist() if c.endswith("L5_variance")]
display_scatter_matrix(matrix_data, "target", "target")
In [81]:
matrix_data = [c for c in df.columns.tolist() if c.endswith("L3_weight")]
display_scatter_matrix(matrix_data, "target", "target")
In [82]:
matrix_data = [c for c in df.columns.tolist() if c.endswith("L3_mean")]
display_scatter_matrix(matrix_data, "target", "target")
In [83]:
matrix_data = [c for c in df.columns.tolist() if c.endswith("L3_variance")]
display_scatter_matrix(matrix_data, "target", "target")
In [84]:
iot_bot_correlation_matrix = df.corr(numeric_only=True).abs()
iot_bot_correlation_matrix.head(20)
Out[84]:
MI_dir_L5_weightMI_dir_L5_meanMI_dir_L5_varianceMI_dir_L3_weightMI_dir_L3_meanMI_dir_L3_varianceMI_dir_L1_weightMI_dir_L1_meanMI_dir_L1_varianceMI_dir_L0.1_weightMI_dir_L0.1_meanMI_dir_L0.1_varianceMI_dir_L0.01_weightMI_dir_L0.01_meanMI_dir_L0.01_varianceH_L5_weightH_L5_meanH_L5_varianceH_L3_weightH_L3_meanH_L3_varianceH_L1_weightH_L1_meanH_L1_varianceH_L0.1_weightH_L0.1_meanH_L0.1_varianceH_L0.01_weightH_L0.01_meanH_L0.01_varianceHH_L5_weightHH_L5_meanHH_L5_stdHH_L5_magnitudeHH_L5_radiusHH_L5_covarianceHH_L5_pccHH_L3_weightHH_L3_meanHH_L3_stdHH_L3_magnitudeHH_L3_radiusHH_L3_covarianceHH_L3_pccHH_L1_weightHH_L1_meanHH_L1_stdHH_L1_magnitudeHH_L1_radiusHH_L1_covarianceHH_L1_pccHH_L0.1_weightHH_L0.1_meanHH_L0.1_stdHH_L0.1_magnitudeHH_L0.1_radiusHH_L0.1_covarianceHH_L0.1_pccHH_L0.01_weightHH_L0.01_meanHH_L0.01_stdHH_L0.01_magnitudeHH_L0.01_radius HH_L0.01_covariance HH_L0.01_pcc HH_jit_L5_weight HH_jit_L5_mean HH_jit_L5_variance HH_jit_L3_weight HH_jit_L3_mean HH_jit_L3_variance HH_jit_L1_weight HH_jit_L1_mean HH_jit_L1_variance HH_jit_L0.1_weight HH_jit_L0.1_mean HH_jit_L0.1_variance HH_jit_L0.01_weight HH_jit_L0.01_mean HH_jit_L0.01_variance HpHp_L5_weight HpHp_L5_mean HpHp_L5_std HpHp_L5_magnitude HpHp_L5_radius HpHp_L5_covariance HpHp_L5_pcc HpHp_L3_weight HpHp_L3_mean HpHp_L3_std HpHp_L3_magnitude HpHp_L3_radius HpHp_L3_covariance HpHp_L3_pcc HpHp_L1_weight HpHp_L1_mean HpHp_L1_std HpHp_L1_magnitude HpHp_L1_radius HpHp_L1_covariance HpHp_L1_pcc HpHp_L0.1_weight HpHp_L0.1_mean HpHp_L0.1_std HpHp_L0.1_magnitude HpHp_L0.1_radius HpHp_L0.1_covariance HpHp_L0.1_pcc HpHp_L0.01_weight HpHp_L0.01_mean HpHp_L0.01_std HpHp_L0.01_magnitude HpHp_L0.01_radius HpHp_L0.01_covariance HpHp_L0.01_pcc MI_dir_L5_weight1.0000000.2331520.3504370.9872730.2701310.3438910.9400770.3143410.3310150.7859320.3302860.3236260.5955670.3324770.3227921.0000000.2331520.3504370.9872730.2701310.3438910.9400770.3143410.3310150.7859320.3302880.3236250.5955670.3324800.3227920.6924310.2092590.0064480.2095700.0086510.0056940.0246130.6932480.2092930.0091240.2095720.0094960.0067210.0354700.6901310.2092610.0348560.2094740.0113570.0113230.0617100.6342900.2088460.0812060.2088840.0121620.0029850.1322470.5524360.2092990.0719160.2094720.010781 0.001258 0.197976 0.692431 0.346420 0.025721 0.693248 0.346525 0.031205 0.690131 0.351062 0.089560 0.634290 0.373703 0.132984 0.552436 0.379138 0.138053 0.059242 0.209117 0.005420 0.183769 0.001743 0.004954 0.004221 0.060433 0.209119 0.005857 0.183772 0.002659 0.005325 0.005030 0.057171 0.209121 0.006037 0.183787 0.003469 0.006344 0.006644 0.052622 0.209127 0.001474 0.183992 0.003397 0.007350 0.005338 0.049185 0.209140 0.003435 0.184144 0.003254 0.006570 0.003449 MI_dir_L5_mean0.2331521.0000000.7607460.2778480.9904650.8288610.3356530.9533620.8901240.5835810.9265790.9114320.6139350.9243530.9128690.2331521.0000000.7607460.2778480.9904650.8288610.3356530.9533620.8901240.5835810.9265790.9114320.6139360.9243530.9128690.0655820.7806090.0217820.7637860.0025750.0008160.0099230.0610190.7806440.0338870.7637640.0019390.0012340.0150020.0633400.7806610.0618130.7637160.0009350.0034090.0278550.2698230.7804900.1474250.7646280.0068480.0099650.0602700.4460330.7803230.2060520.7657310.008688 0.010996 0.083156 0.065582 0.187116 0.008677 0.061019 0.187199 0.001593 0.063340 0.191115 0.083276 0.269823 0.210575 0.124286 0.446033 0.214980 0.126105 0.372403 0.780341 0.004441 0.768995 0.003113 0.000488 0.000189 0.370943 0.780346 0.004570 0.769002 0.003356 0.000591 0.000452 0.366168 0.780351 0.005622 0.769017 0.003938 0.001020 0.001302 0.365688 0.780318 0.010253 0.770138 0.009672 0.008464 0.008544 0.357954 0.780297 0.010972 0.770903 0.009585 0.008753 0.010811 MI_dir_L5_variance0.3504370.7607461.0000000.3526920.8271620.9868100.3580300.8987280.9523110.5906860.9229640.9344520.6347310.9244280.9332200.3504370.7607461.0000000.3526920.8271620.9868100.3580300.8987280.9523110.5906860.9229660.9344520.6347310.9244310.9332200.0571040.6020270.0493610.5802620.0039540.0016680.0090750.0551360.6020030.0616910.5801890.0063030.0021180.0126880.0429830.6019460.0774830.5800670.0095010.0042700.0252920.1349680.6017590.1304620.5806170.0031480.0105700.0595540.2898640.6019120.1700670.5816860.003246 0.011713 0.089518 0.057104 0.009678 0.007155 0.055136 0.009744 0.013914 0.042983 0.013071 0.092839 0.134968 0.030298 0.129022 0.289864 0.034387 0.130530 0.290756 0.602055 0.004894 0.587626 0.003278 0.001340 0.001025 0.306664 0.602059 0.004956 0.587631 0.003470 0.001477 0.001336 0.323371 0.602064 0.006048 0.587644 0.004026 0.001918 0.002197 0.324060 0.602024 0.011055 0.588536 0.010298 0.009057 0.009348 0.320563 0.602004 0.012205 0.589149 0.010214 0.009349 0.013117 MI_dir_L3_weight0.9872730.2778480.3526921.0000000.3062310.3526210.9785380.3399180.3452420.8166170.3515180.3403100.5882920.3534460.3399300.9872730.2778480.3526921.0000000.3062310.3526210.9785380.3399180.3452420.8166170.3515200.3403100.5882920.3534490.3399300.7522470.2472430.0051900.2495030.0085780.0056550.0242590.7554860.2472780.0070310.2495040.0093790.0066680.0348900.7554520.2472500.0313820.2494080.0111800.0112210.0609280.7017130.2468480.0707450.2489060.0119660.0029150.1309760.6170780.2472850.0581120.2495510.010962 0.001074 0.196153 0.752247 0.409177 0.025746 0.755486 0.409295 0.031946 0.755452 0.414481 0.099847 0.701713 0.440349 0.150097 0.617078 0.446500 0.155588 0.068950 0.247090 0.005581 0.221638 0.001923 0.004923 0.004198 0.072198 0.247093 0.005970 0.221641 0.002767 0.005291 0.005000 0.071633 0.247094 0.006150 0.221657 0.003527 0.006299 0.006595 0.066916 0.247097 0.002121 0.221918 0.002790 0.007247 0.005355 0.063937 0.247107 0.004100 0.222109 0.002616 0.006362 0.003515 MI_dir_L3_mean0.2701310.9904650.8271620.3062311.0000000.8850340.3552390.9845430.9353930.6095770.9659190.9519260.6410170.9642050.9530400.2701310.9904650.8271620.3062311.0000000.8850340.3552390.9845430.9353930.6095770.9659190.9519270.6410170.9642050.9530400.0371570.7508860.0188650.7329510.0033210.0011020.0106260.0335010.7509260.0294590.7329370.0029560.0015340.0163170.0370320.7509530.0532330.7328990.0023920.0037990.0299350.2401840.7507910.1306090.7337810.0094100.0103410.0633940.4143240.7506320.1873330.7348490.010657 0.011422 0.087173 0.037157 0.147416 0.008412 0.033501 0.147497 0.001038 0.037032 0.151361 0.086730 0.240184 0.170876 0.129286 0.414324 0.175373 0.131350 0.350338 0.750612 0.004773 0.738916 0.003338 0.000773 0.000474 0.349281 0.750617 0.004900 0.738923 0.003597 0.000883 0.000744 0.344571 0.750622 0.005979 0.738938 0.004197 0.001293 0.001566 0.343877 0.750586 0.010732 0.740024 0.010080 0.008822 0.008937 0.336222 0.750564 0.011481 0.740766 0.009987 0.009119 0.011301 MI_dir_L3_variance0.3438910.8288610.9868100.3526210.8850341.0000000.3672590.9430230.9861850.6142040.9617030.9733290.6611960.9628090.9723610.3438910.8288610.9868100.3526210.8850341.0000000.3672590.9430230.9861850.6142040.9617060.9733290.6611960.9628120.9723610.0386290.6677620.0406610.6452950.0011390.0019220.0101430.0369540.6677630.0538550.6452440.0028290.0023970.0148490.0250880.6677390.0733470.6451530.0052060.0046600.0280170.1683500.6676580.1309120.6458920.0025010.0108140.0630090.3363570.6677790.1765650.6470200.006976 0.012054 0.094017 0.038629 0.041912 0.009984 0.036954 0.041986 0.016963 0.025088 0.045538 0.095834 0.168350 0.063684 0.132568 0.336357 0.067966 0.134265 0.324891 0.667661 0.005352 0.652491 0.003622 0.001582 0.001256 0.340739 0.667666 0.005431 0.652497 0.003859 0.001730 0.001587 0.357992 0.667671 0.006562 0.652511 0.004449 0.002173 0.002456 0.360003 0.667630 0.011620 0.653493 0.010739 0.009431 0.009764 0.356125 0.667610 0.012803 0.654167 0.010644 0.009732 0.013671 MI_dir_L1_weight0.9400770.3356530.3580300.9785380.3552390.3672591.0000000.3756550.3681290.8653260.3815400.3661950.6108460.3831190.3664860.9400770.3356530.3580300.9785380.3552390.3672591.0000000.3756550.3681290.8653260.3815420.3661950.6108460.3831220.3664860.7715310.2794410.0045250.2819890.0091630.0055610.0236240.7781120.2794770.0053640.2819880.0098730.0065440.0339620.7900140.2794550.0278540.2818940.0115370.0109640.0592930.7608970.2790530.0608960.2814920.0139060.0018370.1279980.6768180.2794600.0451710.2821960.013421 0.000683 0.191884 0.771531 0.436949 0.025608 0.778112 0.437083 0.033254 0.790014 0.442973 0.116071 0.760897 0.473129 0.179428 0.676818 0.480354 0.185828 0.071900 0.279276 0.006460 0.253777 0.002631 0.004845 0.004136 0.076537 0.279279 0.006828 0.253782 0.003465 0.005207 0.004924 0.080334 0.279282 0.007154 0.253798 0.004234 0.006194 0.006516 0.078029 0.279263 0.005320 0.254133 0.000124 0.005994 0.004579 0.076185 0.279264 0.007297 0.254380 0.000418 0.004809 0.004410 MI_dir_L1_mean0.3143410.9533620.8987280.3399180.9845430.9430230.3756551.0000000.9752690.6305910.9956410.9834810.6629490.9948520.9840220.3143410.9533620.8987280.3399180.9845430.9430230.3756551.0000000.9752690.6305910.9956410.9834810.6629490.9948520.9840220.0122290.7291700.0171710.7104040.0039260.0014820.0111630.0106200.7292210.0264490.7104010.0039090.0019270.0174410.0170170.7292670.0450060.7103840.0039190.0042210.0315110.2183950.7291470.1174990.7112700.0115310.0105500.0644140.3916250.7289970.1736020.7123020.012111 0.011660 0.088397 0.012229 0.117745 0.007945 0.010620 0.117826 0.000385 0.017017 0.121631 0.088875 0.218395 0.141152 0.132797 0.391625 0.145723 0.135121 0.320737 0.728893 0.005058 0.716916 0.003545 0.001162 0.000869 0.323990 0.728899 0.005145 0.716923 0.003784 0.001287 0.001167 0.324395 0.728904 0.006177 0.716938 0.004323 0.001663 0.001934 0.323778 0.728866 0.011056 0.717998 0.010379 0.009088 0.009190 0.317273 0.728844 0.011840 0.718723 0.010284 0.009392 0.011671 MI_dir_L1_variance0.3310150.8901240.9523110.3452420.9353930.9861850.3681290.9752691.0000000.6249660.9847200.9965210.6739800.9852190.9959650.3310150.8901240.9523110.3452420.9353930.9861850.3681290.9752701.0000000.6249660.9847220.9965210.6739800.9852220.9959650.0224360.7107390.0297450.6878440.0010910.0021600.0112080.0223550.7107740.0424790.6878280.0004970.0026530.0169820.0128480.7108010.0664140.6877870.0003580.0049860.0305240.1887560.7108320.1257000.6886930.0085790.0108070.0652800.3646460.7109090.1753680.6898340.010736 0.012024 0.096676 0.022436 0.064844 0.012927 0.022355 0.064923 0.019801 0.012848 0.068491 0.094227 0.188756 0.086822 0.131750 0.364646 0.091183 0.133903 0.365686 0.710516 0.005662 0.694906 0.003868 0.001813 0.001481 0.376239 0.710521 0.005730 0.694912 0.004116 0.001973 0.001836 0.387450 0.710527 0.006851 0.694927 0.004690 0.002413 0.002703 0.390148 0.710486 0.011941 0.695968 0.010995 0.009648 0.009992 0.384361 0.710466 0.013144 0.696682 0.010894 0.009955 0.013975 MI_dir_L0.1_weight0.7859320.5835810.5906860.8166170.6095770.6142040.8653260.6305910.6249661.0000000.6345100.6255150.8743660.6357360.6259430.7859320.5835810.5906860.8166170.6095770.6142040.8653260.6305910.6249661.0000000.6345120.6255150.8743660.6357390.6259430.4633680.4616540.0000970.4469600.0082960.0054220.0214320.4678670.4617010.0022370.4469600.0088500.0063130.0306470.4957690.4617000.0109360.4468790.0102320.0102430.0541660.6710570.4613970.0172150.4468270.0149370.0009300.1229170.7088540.4617610.0148750.4477550.014661 0.003524 0.185810 0.463368 0.285265 0.026405 0.467867 0.285405 0.036368 0.495769 0.291365 0.137010 0.671057 0.320865 0.191890 0.708854 0.327757 0.195924 0.158766 0.461430 0.009461 0.435524 0.005788 0.004752 0.004057 0.163850 0.461436 0.009684 0.435531 0.006410 0.005106 0.004826 0.169398 0.461441 0.010091 0.435550 0.006986 0.006073 0.006430 0.171599 0.461416 0.008625 0.436159 0.003133 0.002786 0.001432 0.171691 0.461411 0.010520 0.436589 0.003327 0.001690 0.007247 MI_dir_L0.1_mean0.3302860.9265790.9229640.3515180.9659190.9617030.3815400.9956410.9847200.6345101.0000000.9887130.6673550.9999080.9889740.3302860.9265790.9229640.3515180.9659190.9617030.3815400.9956410.9847200.6345101.0000000.9887130.6673550.9999080.9889740.0047850.7251880.0170850.7063660.0046390.0017330.0103650.0044120.7252430.0263380.7063660.0047960.0021360.0168800.0128730.7253130.0456380.7063690.0050470.0042130.0292310.2149690.7253910.1210080.7074070.0122520.0103420.0549860.3885910.7253250.1777030.7084640.012416 0.011562 0.077300 0.004785 0.113584 0.008286 0.004412 0.113664 0.000740 0.012873 0.117442 0.088555 0.214969 0.137003 0.133586 0.388591 0.141622 0.136141 0.304386 0.724911 0.005041 0.712760 0.003639 0.001460 0.001206 0.310857 0.724916 0.005116 0.712767 0.003874 0.001577 0.001476 0.316565 0.724922 0.006115 0.712781 0.004379 0.001926 0.002168 0.317721 0.724884 0.011008 0.713835 0.010363 0.009094 0.009213 0.312424 0.724862 0.011874 0.714557 0.010266 0.009390 0.011954 MI_dir_L0.1_variance0.3236260.9114320.9344520.3403100.9519260.9733290.3661950.9834810.9965210.6255150.9887131.0000000.6749940.9889270.9998960.3236260.9114320.9344520.3403100.9519260.9733290.3661950.9834810.9965210.6255150.9887151.0000000.6749940.9889300.9998960.0188770.7170060.0234840.6940770.0039990.0023370.0114250.0195760.7170600.0352490.6940760.0041180.0027940.0176360.0114200.7171180.0598770.6940630.0043310.0050670.0310640.1909230.7171860.1210330.6950210.0123670.0110410.0652670.3679420.7172280.1719190.6961380.012841 0.012269 0.096537 0.018877 0.067642 0.013331 0.019576 0.067720 0.020029 0.011420 0.071200 0.091597 0.190923 0.089634 0.132379 0.367942 0.094095 0.135053 0.378580 0.716732 0.005659 0.701158 0.003981 0.001993 0.001682 0.387086 0.716738 0.005735 0.701165 0.004236 0.002130 0.002005 0.395058 0.716744 0.006839 0.701180 0.004793 0.002561 0.002840 0.396882 0.716704 0.011835 0.702227 0.010908 0.009600 0.009969 0.390286 0.716684 0.013021 0.702946 0.010803 0.009892 0.013837 MI_dir_L0.01_weight0.5955670.6139350.6347310.5882920.6410170.6611960.6108460.6629490.6739800.8743660.6673550.6749941.0000000.6684670.6749410.5955670.6139350.6347310.5882920.6410170.6611960.6108460.6629490.6739800.8743660.6673580.6749941.0000000.6684700.6749410.1460020.4842050.0104530.4562890.0042420.0053100.0160210.1480280.4842440.0141570.4562880.0045850.0060220.0207450.1707780.4842360.0029270.4562060.0056910.0088760.0400420.3957020.4839400.0095450.4562220.0116850.0053000.1133680.5246880.4843340.0249170.4572000.011448 0.006414 0.177229 0.146002 0.066250 0.026689 0.148028 0.066340 0.033416 0.170778 0.069538 0.087486 0.395702 0.085468 0.114994 0.524688 0.089233 0.116732 0.192855 0.483999 0.008832 0.458032 0.005472 0.004719 0.004025 0.200264 0.484003 0.008889 0.458038 0.005806 0.005071 0.004784 0.209381 0.484007 0.009452 0.458056 0.006326 0.006034 0.006491 0.216665 0.483979 0.009551 0.458723 0.005575 0.002114 0.004011 0.225239 0.483972 0.011585 0.459193 0.005682 0.002600 0.011963 MI_dir_L0.01_mean0.3324770.9243530.9244280.3534460.9642050.9628090.3831190.9948520.9852190.6357360.9999080.9889270.6684671.0000000.9891920.3324770.9243530.9244280.3534460.9642050.9628090.3831190.9948520.9852190.6357360.9999080.9889270.6684671.0000000.9891920.0054770.7249490.0171510.7059890.0047340.0018470.0103610.0051780.7250050.0264430.7059900.0048960.0022260.0168540.0137700.7250790.0465730.7059950.0051460.0042290.0284780.2158520.7252130.1225040.7070750.0122320.0102130.0525350.3893690.7251760.1790690.7081460.012327 0.011479 0.075100 0.005477 0.112764 0.008531 0.005178 0.112843 0.000962 0.013770 0.116631 0.088860 0.215852 0.136218 0.133837 0.389369 0.140835 0.136338 0.303454 0.724673 0.004914 0.712277 0.003624 0.001574 0.001326 0.310068 0.724678 0.004995 0.712283 0.003858 0.001673 0.001572 0.316069 0.724684 0.005986 0.712297 0.004355 0.002014 0.002245 0.317377 0.724647 0.010876 0.713350 0.010281 0.009034 0.009171 0.312153 0.724625 0.011759 0.714071 0.010185 0.009321 0.011850 MI_dir_L0.01_variance0.3227920.9128690.9332200.3399300.9530400.9723610.3664860.9840220.9959650.6259430.9889740.9998960.6749410.9891921.0000000.3227920.9128690.9332200.3399300.9530400.9723610.3664860.9840220.9959650.6259430.9889760.9998960.6749410.9891951.0000000.0179160.7172130.0230770.6944090.0044870.0023630.0116200.0186430.7172670.0347890.6944070.0046340.0027990.0179970.0105550.7173260.0593560.6943950.0048870.0050660.0314330.1917140.7173950.1206570.6953530.0126940.0109380.0654020.3686270.7174320.1716770.6964670.012870 0.012167 0.096442 0.017916 0.068047 0.013247 0.018643 0.068126 0.020121 0.010555 0.071699 0.094103 0.191714 0.090515 0.135137 0.368627 0.095033 0.137616 0.378669 0.716936 0.005517 0.701476 0.003894 0.002021 0.001719 0.387119 0.716941 0.005591 0.701483 0.004139 0.002138 0.002016 0.394982 0.716947 0.006664 0.701497 0.004675 0.002561 0.002834 0.396752 0.716909 0.011538 0.702542 0.010671 0.009426 0.009795 0.390135 0.716888 0.012630 0.703259 0.010570 0.009696 0.013225 H_L5_weight1.0000000.2331520.3504370.9872730.2701310.3438910.9400770.3143410.3310150.7859320.3302860.3236260.5955670.3324770.3227921.0000000.2331520.3504370.9872730.2701310.3438910.9400770.3143410.3310150.7859320.3302880.3236250.5955670.3324800.3227920.6924310.2092590.0064480.2095700.0086510.0056940.0246130.6932480.2092930.0091240.2095720.0094960.0067210.0354700.6901310.2092610.0348560.2094740.0113570.0113230.0617100.6342900.2088460.0812060.2088840.0121620.0029850.1322470.5524360.2092990.0719160.2094720.010781 0.001258 0.197976 0.692431 0.346420 0.025721 0.693248 0.346525 0.031205 0.690131 0.351062 0.089560 0.634290 0.373703 0.132984 0.552436 0.379138 0.138053 0.059242 0.209117 0.005420 0.183769 0.001743 0.004954 0.004221 0.060433 0.209119 0.005857 0.183772 0.002659 0.005325 0.005030 0.057171 0.209121 0.006037 0.183787 0.003469 0.006344 0.006644 0.052622 0.209127 0.001474 0.183992 0.003397 0.007350 0.005338 0.049185 0.209140 0.003435 0.184144 0.003254 0.006570 0.003449 H_L5_mean0.2331521.0000000.7607460.2778480.9904650.8288610.3356530.9533620.8901240.5835810.9265790.9114320.6139350.9243530.9128690.2331521.0000000.7607460.2778480.9904650.8288610.3356530.9533620.8901240.5835810.9265790.9114320.6139360.9243530.9128690.0655820.7806090.0217820.7637860.0025750.0008160.0099230.0610190.7806440.0338870.7637640.0019390.0012340.0150020.0633400.7806610.0618130.7637160.0009350.0034090.0278550.2698230.7804900.1474250.7646280.0068480.0099650.0602700.4460330.7803230.2060520.7657310.008688 0.010996 0.083156 0.065582 0.187116 0.008677 0.061019 0.187199 0.001593 0.063340 0.191115 0.083276 0.269823 0.210575 0.124286 0.446033 0.214980 0.126105 0.372403 0.780341 0.004441 0.768995 0.003113 0.000488 0.000189 0.370943 0.780346 0.004570 0.769002 0.003356 0.000591 0.000452 0.366168 0.780351 0.005622 0.769017 0.003938 0.001020 0.001302 0.365688 0.780318 0.010253 0.770138 0.009672 0.008464 0.008544 0.357954 0.780297 0.010972 0.770903 0.009585 0.008753 0.010811 H_L5_variance0.3504370.7607461.0000000.3526920.8271620.9868100.3580300.8987280.9523110.5906860.9229640.9344520.6347310.9244280.9332200.3504370.7607461.0000000.3526920.8271620.9868100.3580300.8987280.9523110.5906860.9229660.9344520.6347310.9244310.9332200.0571040.6020270.0493610.5802620.0039540.0016680.0090750.0551360.6020030.0616910.5801890.0063030.0021180.0126880.0429830.6019460.0774830.5800670.0095010.0042700.0252920.1349680.6017590.1304620.5806170.0031480.0105700.0595540.2898640.6019120.1700670.5816860.003246 0.011713 0.089518 0.057104 0.009678 0.007155 0.055136 0.009744 0.013914 0.042983 0.013071 0.092839 0.134968 0.030298 0.129022 0.289864 0.034387 0.130530 0.290756 0.602055 0.004894 0.587626 0.003278 0.001340 0.001025 0.306664 0.602059 0.004956 0.587631 0.003470 0.001477 0.001336 0.323371 0.602064 0.006048 0.587644 0.004026 0.001918 0.002197 0.324060 0.602024 0.011055 0.588536 0.010298 0.009057 0.009348 0.320563 0.602004 0.012205 0.589149 0.010214 0.009349 0.013117 H_L3_weight0.9872730.2778480.3526921.0000000.3062310.3526210.9785380.3399180.3452420.8166170.3515180.3403100.5882920.3534460.3399300.9872730.2778480.3526921.0000000.3062310.3526210.9785380.3399180.3452420.8166170.3515200.3403100.5882920.3534490.3399300.7522470.2472430.0051900.2495030.0085780.0056550.0242590.7554860.2472780.0070310.2495040.0093790.0066680.0348900.7554520.2472500.0313820.2494080.0111800.0112210.0609280.7017130.2468480.0707450.2489060.0119660.0029150.1309760.6170780.2472850.0581120.2495510.010962 0.001074 0.196153 0.752247 0.409177 0.025746 0.755486 0.409295 0.031946 0.755452 0.414481 0.099847 0.701713 0.440349 0.150097 0.617078 0.446500 0.155588 0.068950 0.247090 0.005581 0.221638 0.001923 0.004923 0.004198 0.072198 0.247093 0.005970 0.221641 0.002767 0.005291 0.005000 0.071633 0.247094 0.006150 0.221657 0.003527 0.006299 0.006595 0.066916 0.247097 0.002121 0.221918 0.002790 0.007247 0.005355 0.063937 0.247107 0.004100 0.222109 0.002616 0.006362 0.003515 H_L3_mean0.2701310.9904650.8271620.3062311.0000000.8850340.3552390.9845430.9353930.6095770.9659190.9519260.6410170.9642050.9530400.2701310.9904650.8271620.3062311.0000000.8850340.3552390.9845430.9353930.6095770.9659190.9519270.6410170.9642050.9530400.0371570.7508860.0188650.7329510.0033210.0011020.0106260.0335010.7509260.0294590.7329370.0029560.0015340.0163170.0370320.7509530.0532330.7328990.0023920.0037990.0299350.2401840.7507910.1306090.7337810.0094100.0103410.0633940.4143240.7506320.1873330.7348490.010657 0.011422 0.087173 0.037157 0.147416 0.008412 0.033501 0.147497 0.001038 0.037032 0.151361 0.086730 0.240184 0.170876 0.129286 0.414324 0.175373 0.131350 0.350338 0.750612 0.004773 0.738916 0.003338 0.000773 0.000474 0.349281 0.750617 0.004900 0.738923 0.003597 0.000883 0.000744 0.344571 0.750622 0.005979 0.738938 0.004197 0.001293 0.001566 0.343877 0.750586 0.010732 0.740024 0.010080 0.008822 0.008937 0.336222 0.750564 0.011481 0.740766 0.009987 0.009119 0.011301 In [85]:
upper = iot_bot_correlation_matrix.where(np.triu(np.ones(iot_bot_correlation_matrix.shape), k=1).astype(bool))
upper.head()
Out[85]:
MI_dir_L5_weightMI_dir_L5_meanMI_dir_L5_varianceMI_dir_L3_weightMI_dir_L3_meanMI_dir_L3_varianceMI_dir_L1_weightMI_dir_L1_meanMI_dir_L1_varianceMI_dir_L0.1_weightMI_dir_L0.1_meanMI_dir_L0.1_varianceMI_dir_L0.01_weightMI_dir_L0.01_meanMI_dir_L0.01_varianceH_L5_weightH_L5_meanH_L5_varianceH_L3_weightH_L3_meanH_L3_varianceH_L1_weightH_L1_meanH_L1_varianceH_L0.1_weightH_L0.1_meanH_L0.1_varianceH_L0.01_weightH_L0.01_meanH_L0.01_varianceHH_L5_weightHH_L5_meanHH_L5_stdHH_L5_magnitudeHH_L5_radiusHH_L5_covarianceHH_L5_pccHH_L3_weightHH_L3_meanHH_L3_stdHH_L3_magnitudeHH_L3_radiusHH_L3_covarianceHH_L3_pccHH_L1_weightHH_L1_meanHH_L1_stdHH_L1_magnitudeHH_L1_radiusHH_L1_covarianceHH_L1_pccHH_L0.1_weightHH_L0.1_meanHH_L0.1_stdHH_L0.1_magnitudeHH_L0.1_radiusHH_L0.1_covarianceHH_L0.1_pccHH_L0.01_weightHH_L0.01_meanHH_L0.01_stdHH_L0.01_magnitudeHH_L0.01_radius HH_L0.01_covariance HH_L0.01_pcc HH_jit_L5_weight HH_jit_L5_mean HH_jit_L5_variance HH_jit_L3_weight HH_jit_L3_mean HH_jit_L3_variance HH_jit_L1_weight HH_jit_L1_mean HH_jit_L1_variance HH_jit_L0.1_weight HH_jit_L0.1_mean HH_jit_L0.1_variance HH_jit_L0.01_weight HH_jit_L0.01_mean HH_jit_L0.01_variance HpHp_L5_weight HpHp_L5_mean HpHp_L5_std HpHp_L5_magnitude HpHp_L5_radius HpHp_L5_covariance HpHp_L5_pcc HpHp_L3_weight HpHp_L3_mean HpHp_L3_std HpHp_L3_magnitude HpHp_L3_radius HpHp_L3_covariance HpHp_L3_pcc HpHp_L1_weight HpHp_L1_mean HpHp_L1_std HpHp_L1_magnitude HpHp_L1_radius HpHp_L1_covariance HpHp_L1_pcc HpHp_L0.1_weight HpHp_L0.1_mean HpHp_L0.1_std HpHp_L0.1_magnitude HpHp_L0.1_radius HpHp_L0.1_covariance HpHp_L0.1_pcc HpHp_L0.01_weight HpHp_L0.01_mean HpHp_L0.01_std HpHp_L0.01_magnitude HpHp_L0.01_radius HpHp_L0.01_covariance HpHp_L0.01_pcc MI_dir_L5_weightNaN0.2331520.3504370.9872730.2701310.3438910.9400770.3143410.3310150.7859320.3302860.3236260.5955670.3324770.3227921.0000000.2331520.3504370.9872730.2701310.3438910.9400770.3143410.3310150.7859320.3302880.3236250.5955670.3324800.3227920.6924310.2092590.0064480.2095700.0086510.0056940.0246130.6932480.2092930.0091240.2095720.0094960.0067210.0354700.6901310.2092610.0348560.2094740.0113570.0113230.0617100.6342900.2088460.0812060.2088840.0121620.0029850.1322470.5524360.2092990.0719160.2094720.010781 0.001258 0.197976 0.692431 0.346420 0.025721 0.693248 0.346525 0.031205 0.690131 0.351062 0.089560 0.634290 0.373703 0.132984 0.552436 0.379138 0.138053 0.059242 0.209117 0.005420 0.183769 0.001743 0.004954 0.004221 0.060433 0.209119 0.005857 0.183772 0.002659 0.005325 0.005030 0.057171 0.209121 0.006037 0.183787 0.003469 0.006344 0.006644 0.052622 0.209127 0.001474 0.183992 0.003397 0.007350 0.005338 0.049185 0.209140 0.003435 0.184144 0.003254 0.006570 0.003449 MI_dir_L5_meanNaNNaN0.7607460.2778480.9904650.8288610.3356530.9533620.8901240.5835810.9265790.9114320.6139350.9243530.9128690.2331521.0000000.7607460.2778480.9904650.8288610.3356530.9533620.8901240.5835810.9265790.9114320.6139360.9243530.9128690.0655820.7806090.0217820.7637860.0025750.0008160.0099230.0610190.7806440.0338870.7637640.0019390.0012340.0150020.0633400.7806610.0618130.7637160.0009350.0034090.0278550.2698230.7804900.1474250.7646280.0068480.0099650.0602700.4460330.7803230.2060520.7657310.008688 0.010996 0.083156 0.065582 0.187116 0.008677 0.061019 0.187199 0.001593 0.063340 0.191115 0.083276 0.269823 0.210575 0.124286 0.446033 0.214980 0.126105 0.372403 0.780341 0.004441 0.768995 0.003113 0.000488 0.000189 0.370943 0.780346 0.004570 0.769002 0.003356 0.000591 0.000452 0.366168 0.780351 0.005622 0.769017 0.003938 0.001020 0.001302 0.365688 0.780318 0.010253 0.770138 0.009672 0.008464 0.008544 0.357954 0.780297 0.010972 0.770903 0.009585 0.008753 0.010811 MI_dir_L5_varianceNaNNaNNaN0.3526920.8271620.9868100.3580300.8987280.9523110.5906860.9229640.9344520.6347310.9244280.9332200.3504370.7607461.0000000.3526920.8271620.9868100.3580300.8987280.9523110.5906860.9229660.9344520.6347310.9244310.9332200.0571040.6020270.0493610.5802620.0039540.0016680.0090750.0551360.6020030.0616910.5801890.0063030.0021180.0126880.0429830.6019460.0774830.5800670.0095010.0042700.0252920.1349680.6017590.1304620.5806170.0031480.0105700.0595540.2898640.6019120.1700670.5816860.003246 0.011713 0.089518 0.057104 0.009678 0.007155 0.055136 0.009744 0.013914 0.042983 0.013071 0.092839 0.134968 0.030298 0.129022 0.289864 0.034387 0.130530 0.290756 0.602055 0.004894 0.587626 0.003278 0.001340 0.001025 0.306664 0.602059 0.004956 0.587631 0.003470 0.001477 0.001336 0.323371 0.602064 0.006048 0.587644 0.004026 0.001918 0.002197 0.324060 0.602024 0.011055 0.588536 0.010298 0.009057 0.009348 0.320563 0.602004 0.012205 0.589149 0.010214 0.009349 0.013117 MI_dir_L3_weightNaNNaNNaNNaN0.3062310.3526210.9785380.3399180.3452420.8166170.3515180.3403100.5882920.3534460.3399300.9872730.2778480.3526921.0000000.3062310.3526210.9785380.3399180.3452420.8166170.3515200.3403100.5882920.3534490.3399300.7522470.2472430.0051900.2495030.0085780.0056550.0242590.7554860.2472780.0070310.2495040.0093790.0066680.0348900.7554520.2472500.0313820.2494080.0111800.0112210.0609280.7017130.2468480.0707450.2489060.0119660.0029150.1309760.6170780.2472850.0581120.2495510.010962 0.001074 0.196153 0.752247 0.409177 0.025746 0.755486 0.409295 0.031946 0.755452 0.414481 0.099847 0.701713 0.440349 0.150097 0.617078 0.446500 0.155588 0.068950 0.247090 0.005581 0.221638 0.001923 0.004923 0.004198 0.072198 0.247093 0.005970 0.221641 0.002767 0.005291 0.005000 0.071633 0.247094 0.006150 0.221657 0.003527 0.006299 0.006595 0.066916 0.247097 0.002121 0.221918 0.002790 0.007247 0.005355 0.063937 0.247107 0.004100 0.222109 0.002616 0.006362 0.003515 MI_dir_L3_meanNaNNaNNaNNaNNaN0.8850340.3552390.9845430.9353930.6095770.9659190.9519260.6410170.9642050.9530400.2701310.9904650.8271620.3062311.0000000.8850340.3552390.9845430.9353930.6095770.9659190.9519270.6410170.9642050.9530400.0371570.7508860.0188650.7329510.0033210.0011020.0106260.0335010.7509260.0294590.7329370.0029560.0015340.0163170.0370320.7509530.0532330.7328990.0023920.0037990.0299350.2401840.7507910.1306090.7337810.0094100.0103410.0633940.4143240.7506320.1873330.7348490.010657 0.011422 0.087173 0.037157 0.147416 0.008412 0.033501 0.147497 0.001038 0.037032 0.151361 0.086730 0.240184 0.170876 0.129286 0.414324 0.175373 0.131350 0.350338 0.750612 0.004773 0.738916 0.003338 0.000773 0.000474 0.349281 0.750617 0.004900 0.738923 0.003597 0.000883 0.000744 0.344571 0.750622 0.005979 0.738938 0.004197 0.001293 0.001566 0.343877 0.750586 0.010732 0.740024 0.010080 0.008822 0.008937 0.336222 0.750564 0.011481 0.740766 0.009987 0.009119 0.011301 In [86]:
threshold = 0.9
In [87]:
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
In [88]:
display(HTML("<h6 class='messagebox messagebrown'>There are {0} columns can be removed in Dataset</h6>".format(len(to_drop))))
There are 89 columns can be removed in Dataset
In [89]:
f,ax = plt.subplots(figsize=(14, 14))
sns.heatmap(df.iloc[:, 0:30].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
Out[89]:
<Axes: >

In [90]:
f,ax = plt.subplots(figsize=(14, 14))
sns.heatmap(df.iloc[:, 31:60].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
Out[90]:
<Axes: >

In [91]:
f,ax = plt.subplots(figsize=(14, 14))
sns.heatmap(df.iloc[:, 61:90].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
Out[91]:
<Axes: >

In [92]:
f,ax = plt.subplots(figsize=(14, 14))
sns.heatmap(df.iloc[:, 91:115].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
Out[92]:
<Axes: >

In [93]:
#df.target=df.target.astype('category').cat.codes
In [94]:
labelencoder = LabelEncoder()
In [95]:
df['target'] = labelencoder.fit_transform(df['target'])
In [96]:
labelencoder.classes_
Out[96]:
array(['benign', 'gafgyt', 'mirai'], dtype=object)
In [97]:
y = df["target"]
In [98]:
X = df.drop(["target"], axis=1)
In [99]:
from sklearn.preprocessing import MinMaxScaler
In [100]:
scaler = MinMaxScaler()
In [101]:
scaler.fit(X)
Out[101]:
MinMaxScaler()
In [102]:
X = scaler.transform(X)
In [103]:
from sklearn.model_selection import train_test_split
In [104]:
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
In [105]:
from sklearn.feature_selection import SelectKBest
In [106]:
from sklearn.feature_selection import chi2
In [107]:
select_feature = SelectKBest(chi2, k=5).fit(X, y)
In [108]:
print('Score list:', select_feature.scores_)
Score list: [4.79125644e+03 1.11964886e+04 9.38212784e+03 5.28787539e+03
 1.72320129e+04 1.05307933e+04 6.73910461e+03 1.74476977e+04
 2.71378695e+04 1.71231272e+04 1.74384718e+04 3.25318314e+04
 1.96275637e+04 1.74787531e+04 3.27847471e+04 4.79125644e+03
 1.11964886e+04 9.38212784e+03 5.28787539e+03 1.72320129e+04
 1.05307933e+04 6.73910446e+03 1.74477005e+04 2.71378691e+04
 1.71231260e+04 1.74385368e+04 3.25318263e+04 1.96275626e+04
 1.74788476e+04 3.27847436e+04 3.18916159e+03 1.03700388e+04
 1.53913599e+00 5.00263548e+03 6.91943571e+00 2.84623904e-03
 1.09933051e-02 3.77098000e+03 1.03712214e+04 5.32231620e+00
 5.00260958e+03 6.82931738e+00 6.22513174e-03 3.51108670e-02
 4.41344319e+03 1.03692884e+04 7.23324153e+01 5.00166169e+03
 6.18787109e+00 5.05523237e-03 2.88008801e-01 2.81895453e+03
 1.03442666e+04 4.48556383e+02 5.41271957e+03 2.00892579e+01
 5.02434409e-03 2.27438248e+01 4.84324121e+03 1.03592882e+04
 4.95237455e+02 5.88238895e+03 3.02515364e+01 2.67123946e-01
 6.49189890e+01 3.18916159e+03 4.79042977e+03 6.68713649e+00
 3.77098000e+03 4.78951600e+03 1.44467791e+01 4.41344319e+03
 4.80016442e+03 1.31642239e+03 2.81895453e+03 4.93996694e+03
 5.97958805e+03 4.84324121e+03 4.98050772e+03 6.53096583e+03
 2.60561497e+03 1.03643970e+04 1.00247443e+01 5.73053746e+03
 3.90411883e+00 2.23484222e-03 4.97668909e-04 3.21383144e+03
 1.03646514e+04 1.07735444e+01 5.73068366e+03 4.68799843e+00
 4.52782551e-03 1.57247359e-03 4.69201515e+03 1.03649071e+04
 1.11275444e+01 5.73169811e+03 5.98524526e+00 2.89780051e-03
 2.40592498e-03 6.16262248e+03 1.03590293e+04 2.46741679e+01
 6.21809192e+03 3.81406574e+01 1.69399715e+01 1.77284277e+01
 4.97328284e+03 1.03569626e+04 2.91647212e+01 6.69842732e+03
 3.72564059e+01 1.97526457e+01 4.27032401e+01]
In [109]:
x_train.shape
Out[109]:
(76059, 115)
In [110]:
fs = SelectKBest(chi2, k=20)
In [111]:
fs.fit(x_train, y_train)
Out[111]:
SelectKBest(k=20, score_func=<function chi2 at 0x000002AF81E6D040>)
In [112]:
x_train_fs = fs.transform(x_train)
In [113]:
x_train_fs.shape
Out[113]:
(76059, 20)
In [114]:
x_test_fs = fs.transform(x_test)
In [115]:
x_test_fs.shape
Out[115]:
(32598, 20)
In [116]:
for i in range(len(fs.scores_)):
	print('Feature %d: %f' % (i, fs.scores_[i]))
Feature 0: 3349.679734
Feature 1: 7866.962512
Feature 2: 6589.589286
Feature 3: 3699.669388
Feature 4: 12100.921490
Feature 5: 7402.497703
Feature 6: 4721.621043
Feature 7: 12245.287801
Feature 8: 19074.807116
Feature 9: 12024.122862
Feature 10: 12237.996750
Feature 11: 22841.487985
Feature 12: 13804.014731
Feature 13: 12265.599437
Feature 14: 23010.436915
Feature 15: 3349.679734
Feature 16: 7866.962512
Feature 17: 6589.589286
Feature 18: 3699.669388
Feature 19: 12100.921491
Feature 20: 7402.497703
Feature 21: 4721.620973
Feature 22: 12245.288995
Feature 23: 19074.806925
Feature 24: 12024.122259
Feature 25: 12238.030577
Feature 26: 22841.484704
Feature 27: 13804.014139
Feature 28: 12265.652286
Feature 29: 23010.434578
Feature 30: 2219.796574
Feature 31: 7300.331462
Feature 32: 1.466018
Feature 33: 3523.663418
Feature 34: 4.123200
Feature 35: 0.003666
Feature 36: 0.005221
Feature 37: 2625.146031
Feature 38: 7301.190272
Feature 39: 4.411183
Feature 40: 3523.655698
Feature 41: 4.032932
Feature 42: 0.007231
Feature 43: 0.018185
Feature 44: 3069.980249
Feature 45: 7299.856958
Feature 46: 52.239939
Feature 47: 3523.108459
Feature 48: 3.518803
Feature 49: 0.005147
Feature 50: 0.184356
Feature 51: 1977.841484
Feature 52: 7282.921467
Feature 53: 307.586687
Feature 54: 3813.231730
Feature 55: 14.058897
Feature 56: 0.004368
Feature 57: 15.693798
Feature 58: 3424.362110
Feature 59: 7293.063811
Feature 60: 340.978532
Feature 61: 4144.706979
Feature 62: 20.724074
Feature 63: 0.213129
Feature 64: 45.304704
Feature 65: 2219.796574
Feature 66: 3350.909443
Feature 67: 4.709791
Feature 68: 2625.146031
Feature 69: 3350.252032
Feature 70: 11.013134
Feature 71: 3069.980249
Feature 72: 3359.288167
Feature 73: 907.390631
Feature 74: 1977.841484
Feature 75: 3463.840709
Feature 76: 4123.090668
Feature 77: 3424.362110
Feature 78: 3492.012699
Feature 79: 4494.807047
Feature 80: 1847.773523
Feature 81: 7296.289050
Feature 82: 8.608050
Feature 83: 4031.585586
Feature 84: 3.024043
Feature 85: 0.003317
Feature 86: 0.000804
Feature 87: 2282.682749
Feature 88: 7296.516030
Feature 89: 9.130353
Feature 90: 4031.705095
Feature 91: 3.816238
Feature 92: 0.006273
Feature 93: 0.002076
Feature 94: 3340.110932
Feature 95: 7296.741237
Feature 96: 9.343056
Feature 97: 4032.491720
Feature 98: 5.205571
Feature 99: 0.003603
Feature 100: 0.002604
Feature 101: 4389.659413
Feature 102: 7292.462122
Feature 103: 17.955991
Feature 104: 4375.111125
Feature 105: 28.487859
Feature 106: 13.394940
Feature 107: 14.582078
Feature 108: 3547.644069
Feature 109: 7291.011921
Feature 110: 21.778915
Feature 111: 4713.373148
Feature 112: 27.794334
Feature 113: 15.440900
Feature 114: 34.473639
In [117]:
model = LogisticRegression(solver='liblinear')
In [118]:
model.fit(x_train_fs, y_train)
Out[118]:
LogisticRegression(solver='liblinear')
In [119]:
yhat = model.predict(x_test_fs)
In [120]:
lr_accuracy = accuracy_score(y_test, yhat) * 100
print('Accuracy: %.2f' % (lr_accuracy))
Accuracy: 93.74
In [121]:
print(classification_report(y_test, yhat))
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      3000
           1       0.92      0.94      0.93     14598
           2       0.94      0.93      0.93     15000

    accuracy                           0.94     32598
   macro avg       0.95      0.95      0.95     32598
weighted avg       0.94      0.94      0.94     32598

In [122]:
cm = confusion_matrix(y_test, yhat)
plt.figure(figsize=(7,7))
sns.heatmap(cm, fmt='.0f', annot=True, linewidths=0.2, linecolor='purple')
plt.xlabel('predicted value')
plt.ylabel('Truth value')
plt.show()

In [ ]:
 
In [ ]:
 
In [123]:
rf = RandomForestClassifier()
In [124]:
rf.fit(x_train_fs, y_train)
Out[124]:
RandomForestClassifier()
In [125]:
yhat = rf.predict(x_test_fs)
In [126]:
rf_accuracy = accuracy_score(y_test, yhat) * 100
print('Accuracy: %.2f' % (rf_accuracy))
Accuracy: 99.98
In [127]:
print(classification_report(y_test, yhat))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      3000
           1       1.00      1.00      1.00     14598
           2       1.00      1.00      1.00     15000

    accuracy                           1.00     32598
   macro avg       1.00      1.00      1.00     32598
weighted avg       1.00      1.00      1.00     32598

In [128]:
cm = confusion_matrix(y_test, yhat)
plt.figure(figsize=(7,7))
sns.heatmap(cm, fmt='.0f', annot=True, linewidths=0.2, linecolor='purple')
plt.xlabel('predicted value')
plt.ylabel('Truth value')
plt.show()

In [ ]:
 
In [129]:
svc = SVC(gamma='auto')
In [132]:
svc.fit(x_train_fs,y_train)
Out[132]:
SVC(gamma='auto')
In [133]:
svm_accuracy = accuracy_score(y_test, yhat)*100
print('Accuracy: %.2f' % (svm_accuracy))
Accuracy: 99.98
In [134]:
print(classification_report(y_test, yhat))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      3000
           1       1.00      1.00      1.00     14598
           2       1.00      1.00      1.00     15000

    accuracy                           1.00     32598
   macro avg       1.00      1.00      1.00     32598
weighted avg       1.00      1.00      1.00     32598

In [135]:
cm = confusion_matrix(y_test, yhat)
plt.figure(figsize=(7,7))
sns.heatmap(cm, fmt='.0f', annot=True, linewidths=0.2, linecolor='purple')
plt.xlabel('predicted value')
plt.ylabel('Truth value')
plt.show()

In [ ]:
 
In [136]:
labels_full=pd.get_dummies(df['target'], prefix='type')
In [137]:
labels=labels_full.values
labels
Out[137]:
array([[False, False,  True],
       [False, False,  True],
       [False, False,  True],
       ...,
       [ True, False, False],
       [ True, False, False],
       [ True, False, False]])
In [138]:
labels.shape[1], y_test.shape
Out[138]:
(3, (32598,))
In [139]:
x_train_fs.shape[1]
Out[139]:
20
In [140]:
y_train,y_test
Out[140]:
(6141    2
 7618    2
 7074    1
 7098    2
 7794    0
        ..
 4813    0
 1081    2
 7540    1
 1305    2
 4296    2
 Name: target, Length: 76059, dtype: int32,
 9313    2
 5226    1
 2936    2
 8389    1
 2068    1
        ..
 5102    2
 4317    1
 8958    2
 7255    1
 8939    1
 Name: target, Length: 32598, dtype: int32)
In [ ]:
 
In [141]:
y_train = to_categorical(y_train, 3)
y_test = to_categorical(y_test, 3)
In [142]:
def create_nn_model():
    model = Sequential()
    model.add(Dense(10, input_dim=x_train_fs.shape[1], activation='relu'))
    model.add(Dense(40, input_dim=x_train_fs.shape[1], activation='relu'))
    model.add(Dense(10, input_dim=x_train_fs.shape[1], activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    model.add(Dense(labels.shape[1],activation='softmax'))
    return model
In [143]:
model = create_nn_model()
In [144]:
model.compile(loss='categorical_crossentropy', optimizer='adam')
In [145]:
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, 
                        patience=5, verbose=1, mode='auto')
In [146]:
model.fit(x_train_fs,y_train,validation_data=(x_test_fs,y_test),
          callbacks=[monitor],verbose=2,epochs=500)
Epoch 1/500
2377/2377 - 4s - loss: 0.2538 - val_loss: 0.0693 - 4s/epoch - 2ms/step
Epoch 2/500
2377/2377 - 2s - loss: 0.0332 - val_loss: 0.0165 - 2s/epoch - 1ms/step
Epoch 3/500
2377/2377 - 3s - loss: 0.0108 - val_loss: 0.0093 - 3s/epoch - 1ms/step
Epoch 4/500
2377/2377 - 2s - loss: 0.0080 - val_loss: 0.0081 - 2s/epoch - 1ms/step
Epoch 5/500
2377/2377 - 3s - loss: 0.0069 - val_loss: 0.0072 - 3s/epoch - 1ms/step
Epoch 6/500
2377/2377 - 3s - loss: 0.0065 - val_loss: 0.0072 - 3s/epoch - 1ms/step
Epoch 7/500
2377/2377 - 3s - loss: 0.0066 - val_loss: 0.0071 - 3s/epoch - 1ms/step
Epoch 8/500
2377/2377 - 2s - loss: 0.0061 - val_loss: 0.0078 - 2s/epoch - 1ms/step
Epoch 9/500
2377/2377 - 2s - loss: 0.0061 - val_loss: 0.0086 - 2s/epoch - 1ms/step
Epoch 9: early stopping
Out[146]:
<keras.callbacks.History at 0x2af90e93190>
In [147]:
x_test.shape
Out[147]:
(32598, 115)
In [148]:
y_pred = model.predict(x_test_fs)
y_pred = np.argmax(y_pred,axis=1)
y_eval = np.argmax(y_test,axis=1)
nn_model1_accuracy = metrics.accuracy_score(y_eval, y_pred) * 100
print("accuracy: {}".format(nn_model1_accuracy))
1019/1019 [==============================] - 1s 746us/step
accuracy: 99.89876679550893
In [149]:
x_train_fs[0]
Out[149]:
array([7.99671422e-06, 3.57034174e-05, 5.99125795e-05, 6.29739722e-06,
       3.75065692e-01, 3.59749131e-05, 8.98368452e-06, 6.38693070e-01,
       4.10029286e-05, 1.14632646e-05, 7.99671422e-06, 3.57034174e-05,
       5.99125795e-05, 6.29739722e-06, 3.75065692e-01, 3.59749131e-05,
       8.98368452e-06, 6.38693070e-01, 4.10029286e-05, 1.14632646e-05])
In [150]:
def create_cnn_lstm_model():
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding='same', input_shape = (x_train_fs.shape[1], 1)))
    model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding='same'))
    model.add(LSTM(32, activation = 'relu', return_sequences=True))
    model.add(LSTM(16, return_sequences=True))  
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(labels.shape[1],activation='softmax'))
    return model
In [151]:
model = create_cnn_lstm_model()
In [152]:
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=["accuracy"])
In [153]:
plot_model(model,to_file="cnn-lstm.png", show_shapes=True, show_dtype=True, show_layer_activations=True)
You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.
In [154]:
model.fit(x_train_fs,y_train,validation_data=(x_test_fs,y_test),
          callbacks=[monitor],verbose=2,epochs=5)
Epoch 1/5
2377/2377 - 32s - loss: 0.0869 - accuracy: 0.9661 - val_loss: 0.0094 - val_accuracy: 0.9987 - 32s/epoch - 13ms/step
Epoch 2/5
2377/2377 - 28s - loss: 0.0139 - accuracy: 0.9968 - val_loss: 0.0062 - val_accuracy: 0.9991 - 28s/epoch - 12ms/step
Epoch 3/5
2377/2377 - 28s - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.0056 - val_accuracy: 0.9991 - 28s/epoch - 12ms/step
Epoch 4/5
2377/2377 - 32s - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.0053 - val_accuracy: 0.9990 - 32s/epoch - 13ms/step
Epoch 5/5
2377/2377 - 29s - loss: 0.0143 - accuracy: 0.9963 - val_loss: 0.0143 - val_accuracy: 0.9982 - 29s/epoch - 12ms/step
Out[154]:
<keras.callbacks.History at 0x2afb0457bb0>
In [155]:
y_pred = model.predict(x_test_fs)
y_pred = np.argmax(y_pred,axis=1)
y_eval = np.argmax(y_test,axis=1)
nn_model2_accuracy = metrics.accuracy_score(y_eval, y_pred) * 100
print("accuracy: {}".format(nn_model2_accuracy))
1019/1019 [==============================] - 4s 4ms/step
accuracy: 99.81593962819805
In [156]:
y_test.argmax(axis=1)
Out[156]:
array([2, 1, 2, ..., 2, 1, 1], dtype=int64)
In [157]:
y_pred
Out[157]:
array([2, 1, 2, ..., 2, 1, 1], dtype=int64)
In [158]:
print(classification_report(y_test.argmax(axis=1), y_pred, target_names=labelencoder.classes_))
              precision    recall  f1-score   support

      benign       1.00      0.99      0.99      3000
      gafgyt       1.00      1.00      1.00     14598
       mirai       1.00      1.00      1.00     15000

    accuracy                           1.00     32598
   macro avg       1.00      1.00      1.00     32598
weighted avg       1.00      1.00      1.00     32598

In [159]:
cm = confusion_matrix(y_test.argmax(axis=1), y_pred)
plt.figure(figsize=(7,7))
ax = plt.subplot()
sns.heatmap(cm, fmt='.0f', annot=True, linewidths=0.2, linecolor='purple', ax=ax)
plt.xlabel('predicted value')
plt.ylabel('Truth value')
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labelencoder.classes_); 
ax.yaxis.set_ticklabels(labelencoder.classes_);
plt.show()

In [160]:
accuracy_avg = (lr_accuracy + rf_accuracy + svm_accuracy + nn_model1_accuracy + nn_model2_accuracy) / 5
In [161]:
display(HTML("<h6 class='messagebox messagelightgreen'>All Models Accuracy Average is  <b>{0}</b></h6>".format(accuracy_avg)))
All Models Accuracy Average is 98.68212773789804
In [ ]:
 
In [ ]:
 
